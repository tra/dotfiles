{"mode":"editor","version":1,"windowDimensions":{"x":18,"y":89,"width":1440,"height":873,"maximized":false},"syntax":{"deserializer":"Syntax","grammarOverridesByPath":{}},"project":{"paths":["/Users/tra/src/advanced-apps.wiki/analysis"],"buffers":[{"text":"\n\n\n# Load Data\n\nFirst download the Fort Collins load data from [this zip file](./analysis/data/ft-collins-history-2011-2012.tar.gz) and unpack it somewhere.  Then set your working directory to the directory where\nyou unpacked the file (above the 'csvs' directory).\n\nThe following block of code will load up all of the CSV files and pull the hour, day-of-year,\nand day-of-week information into their own columns.\n\n\n```r\n# set working directory to wherever you have unpacked\n# ft-collins-history-2011-2012.tar.gz\nsetwd(\"~/src/advanced-apps.wiki/analysis/data/fcu\")\n\nloads = NULL\nfor (year in 2011:2013) {\n    for (month in 1:12) {\n        filename = paste0(\"./csvs/\", month, \"-\", year, \".csv\")\n        cat(\"loading \", filename, \"\\n\")\n        data = read.csv(filename)\n        # convert garbage characters to integer\n        if (is.factor(data$value)) {\n            data$value = as.integer(as.character(data$value))\n        }\n        # get rid of stuff that looks bad\n        data = data[!is.na(data$value) & data$value > 0, ]\n        \n        # ignore minutes:seconds-tz part, truncate to the hour, then add hour to get\n        # to hour-ending time also, keep times in MST to avoid hour shift at DST\n        # boundaries (considered using UTC but that would put peaks around 0300\n        # which will probably be bad)\n        stimes = gsub(\"(T..):..:..(-..):(..)$\", \"\\\\1\\\\2\\\\3\", data$collectedAt)\n        hours = strptime(stimes, \"%Y-%m-%dT%H%z\", tz = \"MST\") + 3600\n        loads.month = aggregate(data$value, list(hours), mean)\n        loads = rbind(loads, loads.month)\n        rm(stimes)\n        rm(hours)\n        rm(data)\n        rm(loads.month)\n    }\n}\n```\n\n```\n## loading  ./csvs/1-2011.csv \n## loading  ./csvs/2-2011.csv \n## loading  ./csvs/3-2011.csv \n## loading  ./csvs/4-2011.csv \n## loading  ./csvs/5-2011.csv \n## loading  ./csvs/6-2011.csv \n## loading  ./csvs/7-2011.csv \n## loading  ./csvs/8-2011.csv \n## loading  ./csvs/9-2011.csv \n## loading  ./csvs/10-2011.csv \n## loading  ./csvs/11-2011.csv \n## loading  ./csvs/12-2011.csv \n## loading  ./csvs/1-2012.csv \n## loading  ./csvs/2-2012.csv \n## loading  ./csvs/3-2012.csv \n## loading  ./csvs/4-2012.csv \n## loading  ./csvs/5-2012.csv \n## loading  ./csvs/6-2012.csv \n## loading  ./csvs/7-2012.csv \n## loading  ./csvs/8-2012.csv \n## loading  ./csvs/9-2012.csv \n## loading  ./csvs/10-2012.csv \n## loading  ./csvs/11-2012.csv \n## loading  ./csvs/12-2012.csv \n## loading  ./csvs/1-2013.csv \n## loading  ./csvs/2-2013.csv \n## loading  ./csvs/3-2013.csv \n## loading  ./csvs/4-2013.csv \n## loading  ./csvs/5-2013.csv \n## loading  ./csvs/6-2013.csv \n## loading  ./csvs/7-2013.csv \n## loading  ./csvs/8-2013.csv \n## loading  ./csvs/9-2013.csv \n## loading  ./csvs/10-2013.csv \n## loading  ./csvs/11-2013.csv \n## loading  ./csvs/12-2013.csv\n```\n\n```r\n\n# do this after looping over the data\ncolnames(loads) = c(\"time\", \"load\")\nloads$hour = as.POSIXlt(loads$time)$hour\nloads$wday = ((as.POSIXlt(loads$time)$wday + 6)%%7) + 1  # put sunday at 7 so it's next to saturday\nloads$yday = as.POSIXlt(loads$time)$yday\nloads$day = 365 * (as.POSIXlt(loads$time)$year - 111) + as.POSIXlt(loads$time)$yday\n```\n\n\nBefore removing the `time` field, save a copy for later (when we merge weather).\n\n```r\nloads.orig = loads\nloads$time = NULL\n```\n\n\n## MAPE Function\n\nFort Collins wants to measure their MAPE from hours ending 11:00 to 21:00.  The following function\nruns the specified model and outputs the MAPE in that time range.\n\n```r\n# test model error using K-folds\nmodel_mape = function(model, formula, data, tdata = NULL, foldon = \"day\", K = 6, \n    normalize = FALSE, verbose = FALSE, weights = NULL, feedforward = FALSE, \n    ...) {\n    uniques = unique(data[, foldon])\n    folds = split(sample(uniques), 1:length(uniques)%%K)\n    sape = 0\n    y = all.vars(formula)[1]  # the name of the thing we are solving for\n    \n    if (is.null(tdata)) {\n        tdata = data\n    }\n    \n    # normalize data between 0 and 1\n    if (normalize) {\n        ndata = sweep(data, 2, apply(data, 2, min), \"-\")\n        ndata = sweep(ndata, 2, apply(ndata, 2, max), \"/\")\n        ntdata = sweep(tdata, 2, apply(data, 2, min), \"-\")\n        ntdata = sweep(ntdata, 2, apply(ndata, 2, max), \"/\")\n        min_y = min(tdata[y])\n        scale_y = max(tdata[y]) - min_y\n    } else {\n        ndata = data\n        ntdata = tdata\n        min_y = 0\n        scale_y = 1\n    }\n    min_hour = 11\n    max_hour = 21\n    \n    total_test_count = 0\n    for (fold in 1:K) {\n        test_rows = tdata[, foldon] %in% unlist(folds[fold])\n        train_rows = !(data[, foldon] %in% unlist(folds[fold]))\n        \n        train_data = subset(ndata, train_rows)\n        test_data = subset(ntdata, test_rows & ntdata$hour >= min_hour & ntdata$hour <= \n            max_hour)\n        test_data_orig = subset(tdata, test_rows & tdata$hour >= min_hour & \n            tdata$hour <= max_hour)\n        \n        if (is.vector(weights)) {\n            # for some reason, weights need to be assigned in a global environment so\n            # they're accessible\n            assign(\"model_mape.weights\", subset(weights, !test_rows), inherits = TRUE)\n            fit = model(formula, data = train_data, weights = model_mape.weights, \n                ...)\n            rm(\"model_mape.weights\", inherits = TRUE)\n        } else {\n            fit = model(formula, data = train_data, ...)\n        }\n        if (feedforward) {\n            for (cur_hour in min_hour:max_hour) {\n                test_hour = subset(test_data, test_data$hour == cur_hour)\n                if (cur_hour > min_hour) {\n                  # test_hour$load.prev = apply(test_hour[,c('day', 'hour')], 1, function(x)\n                  # test_data[test_data$day==x['day']&test_data$hour==(cur_hour-1),]$prediction[1])\n                  test_hour$load.prev = apply(test_hour[, c(\"day\", \"hour\")], \n                    1, function(x) test_data[test_data$day == x[\"day\"] & test_data$hour == \n                      (cur_hour - 1), ]$prediction[1])\n                }\n                cur_predictions = predict(fit, test_hour, distribution = \"gaussian\", \n                  n.trees = 1000, interaction.depth = 4, shrinkage = 0.2, verbose = TRUE)\n                # put the predictions on test_data so that we can get them in the correct\n                # order later\n                test_data$prediction[test_data$hour == cur_hour] = cur_predictions\n            }\n            predictions = test_data$prediction\n        } else {\n            predictions = predict(fit, test_data, ...)\n        }\n        norm_residuals = (predictions - test_data[y])\n        # denormalize\n        residuals = norm_residuals * scale_y\n        \n        # sum of absolute percentage errors for fold K\n        sape_k = sum(abs(residuals/test_data_orig[y]))\n        sape = sape + sape_k\n        test_count = nrow(test_data[y])\n        total_test_count = total_test_count + test_count\n        if (verbose) {\n            cat(\"mape[\", fold, \"] = \", round(100 * sape_k/test_count, 2), \"\\n\", \n                sep = \"\")\n        }\n    }\n    # mean absolute percentage error (MAPE)\n    round(100 * sape/total_test_count, 2)\n}\n```\n\nAnd since `neuralnet` doesn't define a predict method, we need to write one.\nThis will get called when `predict` is called with the result of a `neuralnet` call.\nSimply put, this gives us an answer that is similar in format to all of the other\n`predict` calls so that we can use it in our model comparisons below.\n\n```r\npredict.nn = function(fit, data, ...) {\n    orig_formula = paste(fit$model.list$response, paste(fit$model.list$variables, \n        collapse = \"+\"), sep = \"~\")\n    # remove the intercept from the original formula since compute doesn't want\n    # that\n    matrix_formula = as.formula(paste(orig_formula, \"- 1\"))\n    datam = model.matrix(matrix_formula, data)\n    result = compute(fit, datam)\n    # return only the last column which is the prediction values\n    predictions = result$net.result[, 1]\n}\n```\n\n\n## Models without Weather\n\nFirst, to get a baseline of our worst models, let's try some simple fits to the data.\n\n\n```r\nrequire(gbm)\nfit = gbm(load ~ ., loads, distribution = \"gaussian\", n.trees = 1000, interaction.depth = 4, \n    shrinkage = 0.2)\nsummary(fit)\n```\n\n<img src=\"analysis/images/gbm-simple.png\" title=\"plot of chunk gbm-simple\" alt=\"plot of chunk gbm-simple\" style=\"display: block; margin: auto;\" />\n\n```\n##       var rel.inf\n## hour hour  57.946\n## yday yday  32.309\n## day   day   6.871\n## wday wday   2.874\n```\n\n<img src=\"analysis/images/gbm-simple-plot.png\" title=\"plot of chunk gbm-simple-plot\" alt=\"plot of chunk gbm-simple-plot\" style=\"display: block; margin: auto;\" />\n\n\n```r\nmodel_mape(gbm, load ~ ., loads, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 4, shrinkage = 0.2)\n```\n\n```\n## [1] 3.96\n```\n\n\nSince we are trying to get the best estimates during peak load, what happens if we weight by\nthe load itself?\n\n```r\nmodel_mape(gbm, load ~ ., loads, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 4, shrinkage = 0.2, weights = loads$load)\n```\n\n```\n## [1] 3.99\n```\n\nNo improvement.  Let's subtract out the minimum and square the result so that the maximum weights have more effect.\n\n```r\nmodel_mape(gbm, load ~ ., loads, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 4, shrinkage = 0.2, weights = (loads$load - min(loads$load))^2)\n```\n\n```\n## [1] 4.2\n```\n\nIt looks like the weights have no discernible effect for `gbm`.\n\n### Adding Yesterday's Load\n\nMany load forecast algorithms use the load from the previous day.\nLet's add that to our feature vector to see how it changes the results.\n\n\n```r\nloads$dayago = apply(loads, 1, function(x) loads[loads$hour == x[\"hour\"] & loads$day == \n    (x[\"day\"] - 1), ]$load[1])\n```\n\n\n\n```r\nfit = gbm(load ~ ., loads, distribution = \"gaussian\", n.trees = 1000, interaction.depth = 4, \n    shrinkage = 0.2)\npar((mfrow = c(1, 1)))\n```\n\n```\n## NULL\n```\n\n```r\nsummary(fit)\n```\n\n<img src=\"analysis/images/yesterday-gbm.png\" title=\"plot of chunk yesterday-gbm\" alt=\"plot of chunk yesterday-gbm\" style=\"display: block; margin: auto;\" />\n\n```\n##           var rel.inf\n## dayago dayago  85.772\n## hour     hour   5.521\n## yday     yday   3.259\n## wday     wday   2.977\n## day       day   2.472\n```\n\nNot surprisingly, the load from the previous day dominates the estimate.  Let's look at MAPE.\n\n\n```r\nmodel_mape(gbm, load ~ ., loads, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 4, shrinkage = 0.2)\n```\n\n```\n## [1] 4.39\n```\n\n```r\n# 4.2\n```\n\n\nWhich is worse than before. Let's try adding week-ago loads too:\n\n```r\nloads$weekago = apply(loads, 1, function(x) loads[loads$hour == x[\"hour\"] & \n    loads$day == (x[\"day\"] - 7), ]$load[1])\nfit = gbm(load ~ ., loads, distribution = \"gaussian\", n.trees = 1000, interaction.depth = 4, \n    shrinkage = 0.2)\nsummary(fit)\n```\n\n<img src=\"analysis/images/lastweek-gbm.png\" title=\"plot of chunk lastweek-gbm\" alt=\"plot of chunk lastweek-gbm\" style=\"display: block; margin: auto;\" />\n\n```\n##             var rel.inf\n## dayago   dayago  73.165\n## weekago weekago  18.200\n## yday       yday   2.845\n## day         day   2.298\n## wday       wday   1.953\n## hour       hour   1.540\n```\n\n\n\n```r\nmodel_mape(gbm, load ~ ., loads, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 4, shrinkage = 0.2)\n```\n\n```\n## [1] 4.32\n```\n\n\nAbout the same. Let's try adding hour-ago loads too:\n\n```r\nloads$hourago = apply(loads, 1, function(x) loads[loads$hour == (x[\"hour\"] - \n    1) & loads$day == x[\"day\"], ]$load[1])\nfit = gbm(load ~ ., loads, distribution = \"gaussian\", n.trees = 1000, interaction.depth = 4, \n    shrinkage = 0.2)\nsummary(fit)\n```\n\n<img src=\"analysis/images/hourago-gbm.png\" title=\"plot of chunk hourago-gbm\" alt=\"plot of chunk hourago-gbm\" style=\"display: block; margin: auto;\" />\n\n```\n##             var rel.inf\n## hourago hourago 75.9012\n## dayago   dayago 16.4347\n## weekago weekago  4.2185\n## hour       hour  2.6735\n## yday       yday  0.4092\n## wday       wday  0.2304\n## day         day  0.1324\n```\n\n\n\n```r\nmodel_mape(gbm, load ~ ., loads, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 4, shrinkage = 0.2)\n```\n\n```\n## [1] 1.25\n```\n\nThat's much better.  For comparison, how does random forests do with this data?\n\nFirst, the `randomForest` package doesn't deal with `NA` data so let's remove those records.\n\n```r\nloads.clean = loads[!is.na(loads$dayago) & !is.na(loads$weekago) & !is.na(loads$hourago), \n    ]\n```\n\n\nAfter trying several settings, the following model is about as good as `randomForest`\ndoes on the data.\nAny number of trees above 80 doesn't appear to improve the results appreciably for this data.\n\n\n```r\nrequire(randomForest)\nmodel_mape(randomForest, load ~ ., loads.clean, ntree = 80, mtry = 5, verbose = FALSE)\n```\n\n```\n## [1] 1.3\n```\n\nThis looks a little worse than boosting and it trains noticeably slower.\n\nBut we might not always have the load from an hour ago.  Can we use the\nprevious hour's estimate to feed forward?  We'll come back to that.\nFirst let's load up some weather data and see how that affects the model.\nFor now, remove hour-ago from the data.\n\n```r\nloads$hourago = NULL\n```\n\n\n# Weather Data\n\n\n```r\n# set working directory to wherever the weather csv files are\nsetwd(\"~/src/advanced-apps.wiki/analysis/data/fcu\")\n\nweathers = NULL\nfor (year in 2011:2013) {\n    filename = paste0(\"./\", year, \"_ft_collins.csv\")\n    cat(\"loading \", filename, \"\\n\")\n    data = read.csv(filename)\n    \n    numeric_fields = c(\"temperature\", \"dew_point\", \"wind_speed\", \"wind_direction\", \n        \"cloud_ceiling\", \"visibility\", \"pressure\")\n    for (field in numeric_fields) {\n        data[field] = as.numeric(as.character(data[field][, 1]))\n    }\n    \n    # ignore minutes:seconds-tz part, truncate to the hour, then add hour to get\n    # to hour-ending time also, keep times in MST to avoid hour shift at DST\n    # boundaries\n    stimes = gsub(\"(T..):..:..([+-]....)$\", \"\\\\1\\\\2\", data$timestamp)\n    hours = strptime(stimes, \"%Y-%m-%dT%H%z\", tz = \"MST\") + 3600\n    weathers.hourly = aggregate(data[, numeric_fields], list(hours), FUN = mean)\n    \n    # take the median sky_cover value\n    weathers.hourly$sky_cover = as.factor(levels(data$sky_cover)[aggregate(as.integer(data$sky_cover), \n        list(hours), median)[, 2]])\n    \n    weathers = rbind(weathers, weathers.hourly)\n    rm(stimes)\n    rm(hours)\n    rm(data)\n    rm(weathers.hourly)\n}\n```\n\n```\n## loading  ./2011_ft_collins.csv \n## loading  ./2012_ft_collins.csv \n## loading  ./2013_ft_collins.csv\n```\n\n```r\ncolnames(weathers)[1] = \"time\"\n```\n\n\nNow merge the `loads` and `weathers` data frames into one big `data.frame` called `features`.\n\n```r\nloads = merge(loads.orig, weathers, by = \"time\")\nloads$time = NULL\n```\n\n\n## Models with Weather\n\nLet's get the same summary information for a boosting model that we got without weather:\n\n```r\nfit = gbm(load ~ ., loads, distribution = \"gaussian\", n.trees = 1000, interaction.depth = 4, \n    shrinkage = 0.2)\npar((mfrow = c(1, 1)))\n```\n\n```\n## NULL\n```\n\n```r\nsummary(fit)\n```\n\n<img src=\"analysis/images/gbm-simple-with-weather.png\" title=\"plot of chunk gbm-simple-with-weather\" alt=\"plot of chunk gbm-simple-with-weather\" style=\"display: block; margin: auto;\" />\n\n```\n##                           var  rel.inf\n## temperature       temperature 58.43780\n## hour                     hour 31.29312\n## yday                     yday  4.04666\n## wday                     wday  2.45888\n## dew_point           dew_point  2.23266\n## day                       day  0.66551\n## pressure             pressure  0.26467\n## wind_speed         wind_speed  0.21055\n## wind_direction wind_direction  0.17385\n## cloud_ceiling   cloud_ceiling  0.09432\n## visibility         visibility  0.06807\n## sky_cover           sky_cover  0.05390\n```\n\n\nAs expected temperature plays a large part in the model.  Plotting the top 3 influences:\n<img src=\"analysis/images/gbm-simple-plot-with-weather.png\" title=\"plot of chunk gbm-simple-plot-with-weather\" alt=\"plot of chunk gbm-simple-plot-with-weather\" style=\"display: block; margin: auto;\" />\n\n\nAnd the MAPE calculation:\n\n\n```r\nmodel_mape(gbm, load ~ ., loads, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 4, shrinkage = 0.2, verbose = TRUE)\n```\n\n```\n## mape[1] = 2.28\n## mape[2] = 2.16\n## mape[3] = 2.43\n## mape[4] = 2.38\n## mape[5] = 2.17\n## mape[6] = 2.23\n```\n\n```\n## [1] 2.27\n```\n\nCompared to the same model without weather, we decreased from 3.7% to 2.2% just by adding weather.\n\nNow let's look at what happens if we add loads from previous times at various time periods.\n\n### Previous Load\n\nLet's try adding yesterday's load back in.\n\n\n```r\nloads$dayago = apply(loads[, c(\"load\", \"hour\", \"day\")], 1, function(x) loads[loads$hour == \n    x[\"hour\"] & loads$day == (x[\"day\"] - 1), ]$load[1])\n```\n\n\n\n\n```r\nfit = gbm(load ~ ., loads, distribution = \"gaussian\", n.trees = 1000, interaction.depth = 4, \n    shrinkage = 0.2)\npar((mfrow = c(1, 1)))\n```\n\n```\n## NULL\n```\n\n```r\nsummary(fit)\n```\n\n<img src=\"analysis/images/yesterday-gbm-with-weather.png\" title=\"plot of chunk yesterday-gbm-with-weather\" alt=\"plot of chunk yesterday-gbm-with-weather\" style=\"display: block; margin: auto;\" />\n\n```\n##                           var  rel.inf\n## dayago                 dayago 66.77172\n## temperature       temperature 21.33792\n## hour                     hour  7.21422\n## wday                     wday  2.25631\n## yday                     yday  0.82040\n## dew_point           dew_point  0.57527\n## day                       day  0.30057\n## pressure             pressure  0.23264\n## wind_speed         wind_speed  0.16724\n## cloud_ceiling   cloud_ceiling  0.10265\n## wind_direction wind_direction  0.08778\n## visibility         visibility  0.08543\n## sky_cover           sky_cover  0.04784\n```\n\n\n\nLet's look at MAPE.\n\n\n```r\nmodel_mape(gbm, load ~ ., loads, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 4, shrinkage = 0.2, verbose = TRUE)\n```\n\n```\n## mape[1] = 2.03\n## mape[2] = 2.21\n## mape[3] = 2.08\n## mape[4] = 2.11\n## mape[5] = 2.56\n## mape[6] = 2.37\n```\n\n```\n## [1] 2.22\n```\n\n```r\n# 2.2\n```\n\nThis is virtually no different than before.\n\nLet's see how a week-ago loads does.\n\n\n```r\nloads$dayago = NULL\nloads$weekago = apply(loads[, c(\"load\", \"hour\", \"day\")], 1, function(x) loads[loads$hour == \n    x[\"hour\"] & loads$day == (x[\"day\"] - 7), ]$load[1])\nmodel_mape(gbm, load ~ ., loads, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 4, shrinkage = 0.2, verbose = TRUE)\n```\n\n```\n## mape[1] = 2.32\n## mape[2] = 2.21\n## mape[3] = 2.5\n## mape[4] = 2.43\n## mape[5] = 2.73\n## mape[6] = 2.32\n```\n\n```\n## [1] 2.42\n```\n\n\nAbout the same. Let's try adding hour-ago loads for various hour increments.\n\n```r\nloads$weekago = NULL\nloads$hour12.ago = apply(loads[, c(\"load\", \"hour\", \"day\")], 1, function(x) loads[loads$hour == \n    (x[\"hour\"] - 12) & loads$day == x[\"day\"], ]$load[1])\nmodel_mape(gbm, load ~ ., loads, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 4, shrinkage = 0.2, verbose = TRUE)\n```\n\n```\n## mape[1] = 2.14\n## mape[2] = 2.12\n## mape[3] = 2.19\n## mape[4] = 2.3\n## mape[5] = 2.11\n## mape[6] = 2.04\n```\n\n```\n## [1] 2.15\n```\n\n```r\n\nloads$hour12.ago = NULL\nloads$hour6.ago = apply(loads[, c(\"load\", \"hour\", \"day\")], 1, function(x) loads[loads$hour == \n    (x[\"hour\"] - 6) & loads$day == x[\"day\"], ]$load[1])\nmodel_mape(gbm, load ~ ., loads, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 4, shrinkage = 0.2, verbose = TRUE)\n```\n\n```\n## mape[1] = 2.06\n## mape[2] = 2\n## mape[3] = 2.06\n## mape[4] = 1.98\n## mape[5] = 1.99\n## mape[6] = 1.91\n```\n\n```\n## [1] 2\n```\n\n```r\n\nloads$hour6.ago = NULL\nloads$hour3.ago = apply(loads[, c(\"load\", \"hour\", \"day\")], 1, function(x) loads[loads$hour == \n    (x[\"hour\"] - 3) & loads$day == x[\"day\"], ]$load[1])\nmodel_mape(gbm, load ~ ., loads, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 4, shrinkage = 0.2, verbose = TRUE)\n```\n\n```\n## mape[1] = 1.7\n## mape[2] = 1.59\n## mape[3] = 1.62\n## mape[4] = 1.58\n## mape[5] = 1.6\n## mape[6] = 1.7\n```\n\n```\n## [1] 1.63\n```\n\n```r\n\nloads$hour3.ago = NULL\nloads$hour1.ago = apply(loads[, c(\"load\", \"hour\", \"day\")], 1, function(x) loads[loads$hour == \n    (x[\"hour\"] - 1) & loads$day == x[\"day\"], ]$load[1])\nmodel_mape(gbm, load ~ ., loads, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 4, shrinkage = 0.2, verbose = TRUE)\n```\n\n```\n## mape[1] = 1.02\n## mape[2] = 1.03\n## mape[3] = 1.04\n## mape[4] = 1.04\n## mape[5] = 0.97\n## mape[6] = 1.03\n```\n\n```\n## [1] 1.02\n```\n\n```r\n\nloads$hour1.ago = NULL\n```\n\n\nThe estimates are best for the case where you have the most recent (1-hour) load.  As expected the accuracy\ndecreases as the time shift increases.  At somewhere around 12-24 hours the estimate with the previous load\nis about the same as the estimate without.  So the model degrades to being just a the long-term load estimate\nwithout knowledge of previous load.\n\n### 10:00AM Load\n\nUnfortunately, we often don't have the 1-hour ago load value.  Specifically, for Fort Collins, they are\nmost interested in the load estimate from 10:00 AM forward.  So let's look at a model that uses the load\nat 10:00 for the daily projection.\n\n\n```r\nloads$load.10am = apply(loads[, c(\"load\", \"hour\", \"day\")], 1, function(x) loads[x[\"hour\"] > \n    10 & loads$hour == 10 & loads$day == x[\"day\"], ]$load[1])\nmodel_mape(gbm, load ~ ., loads, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 4, shrinkage = 0.2, verbose = TRUE)\n```\n\n```\n## mape[1] = 1.86\n## mape[2] = 2.04\n## mape[3] = 1.75\n## mape[4] = 1.87\n## mape[5] = 1.95\n## mape[6] = 1.79\n```\n\n```\n## [1] 1.88\n```\n\n```r\nloads$load.10am = NULL\n```\n\n\nThe MAPE is between the 3-hour and 6-hour estimates from before which is reasonable since we are doing\nestimates for a 10-hour window between 11:00 and 21:00.  So on average you would expect it to be something\nlike a 5-hour estimate.\n\n## Feed-Forward Load\n\nWhat if we just feed forward estimates from one hour to the next?\nIn order to study this idea, `model_mape` had to be modified to do the feed-forward process.\n\n\n```r\nloads$load.prev = apply(loads[, c(\"load\", \"hour\", \"day\")], 1, function(x) loads[loads$hour == \n    (x[\"hour\"] - 1) & loads$day == x[\"day\"], ]$load[1])\n```\n\n\nWithout the `feedforward` flag we should get the same MAPE as the `hours.ago` model above. \n\n```r\nmodel_mape(gbm, load ~ ., loads, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 4, shrinkage = 0.2, verbose = TRUE)\n```\n\n```\n## mape[1] = 1.07\n## mape[2] = 0.99\n## mape[3] = 1.03\n## mape[4] = 1.01\n## mape[5] = 1.01\n## mape[6] = 0.94\n```\n\n```\n## [1] 1.01\n```\n\nAnd with the `feedforward` flag, the error should increase.\n\n```r\nmodel_mape(gbm, load ~ ., loads, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 4, shrinkage = 0.2, feedforward = TRUE, verbose = TRUE)\n```\n\n```\n## mape[1] = 2.08\n## mape[2] = 1.86\n## mape[3] = 1.78\n## mape[4] = 1.96\n## mape[5] = 1.87\n## mape[6] = 1.93\n```\n\n```\n## [1] 1.91\n```\n\n```r\n# 1.9\n```\n\n\n\n```r\nloads$load.prev = NULL\n```\n\n\n### Multiple Training\n\nNext we explore training with multiple instances of every feature vector.\nFor the first example, we take each feature and we add a a `load.prev` sample with the value of the previous load.\nThen we duplicate that record with an `NA` as the value.  That way, we can train the algorithm both for the\ncase where we have a previous load, and the case where we don't.\n\nSo we don't pollute the original data, let's create a new data.frame called `loads.multi`\n\n```r\nloads.prev = loads\nloads.prev$load.prev = apply(loads[, c(\"hour\", \"day\")], 1, function(x) loads[loads$hour == \n    (x[\"hour\"] - 1) & loads$day == x[\"day\"], ]$load[1])\n\nloads.prev.na = loads.prev\nloads.prev.na$load.prev = NA\n\nloads.multi = rbind(loads.prev.na, loads.prev)\n```\n\n\nRunning this with the data set that has no previous load we would expect to be around the same MAPE as\nwe got above with no previous loads (approx. 2.2%):\n\n```r\nmodel_mape(gbm, load ~ ., loads.multi, tdata = loads.prev.na, distribution = \"gaussian\", \n    n.trees = 1000, interaction.depth = 4, shrinkage = 0.2, verbose = TRUE)\n```\n\n```\n## mape[1] = 2.01\n## mape[2] = 2.34\n## mape[3] = 2.41\n## mape[4] = 2.16\n## mape[5] = 2.44\n## mape[6] = 2.27\n```\n\n```\n## [1] 2.27\n```\n\nIdeally using the 1-hour previous load data for testing would get us close to the 1-hour MAPE (approx 1.1%).\n\n```r\nmodel_mape(gbm, load ~ ., loads.multi, tdata = loads.prev, distribution = \"gaussian\", \n    n.trees = 1000, interaction.depth = 5, shrinkage = 0.2, verbose = TRUE)\n```\n\n```\n## mape[1] = 1.42\n## mape[2] = 1.43\n## mape[3] = 1.53\n## mape[4] = 1.48\n## mape[5] = 1.37\n## mape[6] = 1.3\n```\n\n```\n## [1] 1.42\n```\n\nUnfortunately, we don't do as well as before.  This seems to suggest that we cannot achieve the same level of \naccuracy with one model to accomodate estimates that use previous loads.  We might need a different model for each\ntime differential up to some threshold (12-24 hours?).  Or perhaps we can use the feed-forward model up to some\nthreshold then switch to a more general model.\n\n\n## Identifying Important Observations\n\nTo get an idea of the relative importance of each observation, do a summary on the `gbm` output.\n\n\n```r\nsummary(gbm(load ~ ., loads.prev, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 5, shrinkage = 0.2))\n```\n\n<img src=\"analysis/images/gbm-summary.png\" title=\"plot of chunk gbm-summary\" alt=\"plot of chunk gbm-summary\" style=\"display: block; margin: auto;\" />\n\n```\n##                           var  rel.inf\n## load.prev           load.prev 86.97112\n## temperature       temperature  5.99805\n## hour                     hour  5.85714\n## dew_point           dew_point  0.34087\n## yday                     yday  0.33140\n## wday                     wday  0.13181\n## pressure             pressure  0.10519\n## day                       day  0.07648\n## cloud_ceiling   cloud_ceiling  0.06635\n## wind_speed         wind_speed  0.04884\n## wind_direction wind_direction  0.04086\n## visibility         visibility  0.01715\n## sky_cover           sky_cover  0.01474\n```\n\n\nFirst let's get the baseline numbers with and without the previous load.\n\n\n```r\nmodel_mape(gbm, load ~ ., loads.prev, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 5, shrinkage = 0.2, verbose = FALSE)\n```\n\n```\n## [1] 0.96\n```\n\n```r\n# 1.0\nmodel_mape(gbm, load ~ . - load.prev, loads.prev, distribution = \"gaussian\", \n    n.trees = 1000, interaction.depth = 5, shrinkage = 0.2, verbose = FALSE)\n```\n\n```\n## [1] 2.24\n```\n\n```r\n# 2.2\n```\n\n\nNow try removing observatrions from the bottom of the list until we see a\nsignificant change in MAPE.  First remove visibility and check the MAPE.\n\n\n```r\nmodel_mape(gbm, load ~ . - visibility, loads.prev, distribution = \"gaussian\", \n    n.trees = 1000, interaction.depth = 5, shrinkage = 0.2, verbose = FALSE)\n```\n\n```\n## [1] 0.96\n```\n\n```r\n# 1.0\nmodel_mape(gbm, load ~ . - visibility - load.prev, loads.prev, distribution = \"gaussian\", \n    n.trees = 1000, interaction.depth = 5, shrinkage = 0.2, verbose = FALSE)\n```\n\n```\n## [1] 2.28\n```\n\n```r\n# 2.2\n```\n\n\nIt looks like visibility has no discernible effect.  Let's remove more things until we see\na noticeable difference.\n\n\n```r\nmodel_mape(gbm, load ~ . - visibility - sky_cover, loads.prev, distribution = \"gaussian\", \n    n.trees = 1000, interaction.depth = 5, shrinkage = 0.2, verbose = FALSE)\n```\n\n```\n## [1] 0.96\n```\n\n```r\n# 1.0\nmodel_mape(gbm, load ~ . - visibility - sky_cover - load.prev, loads.prev, distribution = \"gaussian\", \n    n.trees = 1000, interaction.depth = 5, shrinkage = 0.2, verbose = FALSE)\n```\n\n```\n## [1] 2.27\n```\n\n```r\n# 2.2\n```\n\n\n\n\n```r\nmodel_mape(gbm, load ~ . - visibility - sky_cover - cloud_ceiling, loads.prev, \n    distribution = \"gaussian\", n.trees = 1000, interaction.depth = 5, shrinkage = 0.2, \n    verbose = FALSE)\n```\n\n```\n## [1] 0.96\n```\n\n```r\n# 1.0\nmodel_mape(gbm, load ~ . - visibility - sky_cover - cloud_ceiling - load.prev, \n    loads.prev, distribution = \"gaussian\", n.trees = 1000, interaction.depth = 5, \n    shrinkage = 0.2, verbose = FALSE)\n```\n\n```\n## [1] 2.27\n```\n\n```r\n# 2.2\n```\n\n\n\n\n```r\nmodel_mape(gbm, load ~ . - visibility - sky_cover - cloud_ceiling - wind_direction - \n    wind_speed, loads.prev, distribution = \"gaussian\", n.trees = 1000, interaction.depth = 5, \n    shrinkage = 0.2, verbose = FALSE)\n```\n\n```\n## [1] 0.96\n```\n\n```r\n# 1.0\nmodel_mape(gbm, load ~ . - visibility - sky_cover - cloud_ceiling - wind_direction - \n    wind_speed - load.prev, loads.prev, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 5, shrinkage = 0.2, verbose = FALSE)\n```\n\n```\n## [1] 2.29\n```\n\n```r\n# 2.2\n```\n\n\n\n```r\nmodel_mape(gbm, load ~ . - visibility - sky_cover - cloud_ceiling - wind_direction - \n    wind_speed - pressure, loads.prev, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 5, shrinkage = 0.2, verbose = FALSE)\n```\n\n```\n## [1] 0.94\n```\n\n```r\n# 1.0\nmodel_mape(gbm, load ~ . - visibility - sky_cover - cloud_ceiling - wind_direction - \n    wind_speed - pressure - load.prev, loads.prev, distribution = \"gaussian\", \n    n.trees = 1000, interaction.depth = 5, shrinkage = 0.2, verbose = FALSE)\n```\n\n```\n## [1] 2.24\n```\n\n```r\n# 2.2\n```\n\n\n\n```r\nmodel_mape(gbm, load ~ . - visibility - sky_cover - cloud_ceiling - wind_direction - \n    wind_speed - pressure - day, loads.prev, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 5, shrinkage = 0.2, verbose = FALSE)\n```\n\n```\n## [1] 0.94\n```\n\n```r\n# 1.0\nmodel_mape(gbm, load ~ . - visibility - sky_cover - cloud_ceiling - wind_direction - \n    wind_speed - pressure - day - load.prev, loads.prev, distribution = \"gaussian\", \n    n.trees = 1000, interaction.depth = 5, shrinkage = 0.2, verbose = FALSE)\n```\n\n```\n## [1] 2.43\n```\n\n```r\n# 2.4\n```\n\n\nWe finally found something that makes a small difference.  Removing `day` makes a small\ndifference for the long-term forecast, which makes some sense since it is\nthe long-term measure of days since the beginning of time.\nBut it makes no difference for the short-term forecast.  This suggests that we might be \nable to use fewer variables for the short-term forecast.\nHowever, since day is always increasing, it might be better to instead model this\nby decreasing the influence of older observations over time.\n\n\n\n```r\nmodel_mape(gbm, load ~ . - visibility - sky_cover - cloud_ceiling - wind_direction - \n    wind_speed - pressure - day - wday, loads.prev, distribution = \"gaussian\", \n    n.trees = 1000, interaction.depth = 5, shrinkage = 0.2, verbose = FALSE)\n```\n\n```\n## [1] 0.98\n```\n\n```r\n# 1.0\nmodel_mape(gbm, load ~ . - visibility - sky_cover - cloud_ceiling - wind_direction - \n    wind_speed - pressure - day - wday - load.prev, loads.prev, distribution = \"gaussian\", \n    n.trees = 1000, interaction.depth = 5, shrinkage = 0.2, verbose = FALSE)\n```\n\n```\n## [1] 3.81\n```\n\n\nAnd now we have our first significant effect on the long-term forecast.\nThis suggests that we need to keep `wday` in the long-term forecast.  And keeping\nit in the short-term forecast has no effect so keeping it might be a decision of\nexpediency.  We might as well keep it for both long-term and short-term forecasts since\nit improves long-term forecasts and has no measurable effect on long-term forecasts.\n\nWe are left with the following observations in order of significance:\n\n * previous load (if available, short-term forecasts only)\n * temperature\n * hour\n * day-of-year (yday)\n * dew point\n * day-of-week (wday)\n \nBreaking this down further by weather and time data.\nThe most important weather data is always in the order of:\n\n  * temperature\n  * dew point\n  \nAnd the most important time data is in the order of:\n\n  * hour\n  * day-of-year\n  * day-of-week\n\n### Tuning the Boosting Model\n\nBased on the last section, let's remove the unused observations and try to tune the `gbm` settings.\n\n```r\nloads.basic = loads.prev\nloads.basic$visibility = NULL\nloads.basic$sky_cover = NULL\nloads.basic$cloud_ceiling = NULL\nloads.basic$wind_direction = NULL\nloads.basic$wind_speed = NULL\nloads.basic$pressure = NULL\n```\n\n\nLet's start with the tunings we have been using up to this point.  We will also tune\nthe short term (with previous load) and long-term models separately.  As much as possible\nwe'd like to tune them the same but that may not always be feasible.\n\nAlso, because the intent is to feed forward the short-term estimates for the most recent load,\nwe will consider use the `feedforward` setting to evaluate the short-term model.\n\n\n```r\nmodel_mape(gbm, load ~ . - day, loads.basic, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 5, shrinkage = 0.2, feedforward = TRUE, verbose = TRUE)\n```\n\n```\n## mape[1] = 1.85\n## mape[2] = 1.8\n## mape[3] = 1.9\n## mape[4] = 1.86\n## mape[5] = 1.92\n## mape[6] = 1.87\n```\n\n```\n## [1] 1.87\n```\n\n```r\n\nmodel_mape(gbm, load ~ . - day - load.prev, loads.basic, distribution = \"gaussian\", \n    n.trees = 1000, interaction.depth = 5, shrinkage = 0.2, verbose = TRUE)\n```\n\n```\n## mape[1] = 2.53\n## mape[2] = 2.43\n## mape[3] = 2.2\n## mape[4] = 2.31\n## mape[5] = 2.64\n## mape[6] = 2.37\n```\n\n```\n## [1] 2.41\n```\n\n\n#### interaction.depth\n\nIncreasing interaction depth increases the complexity of the interactions between observations.\nHere are runs with the short-term model.\n\n\n```r\nmodel_mape(gbm, load ~ . - day, loads.basic, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 3, shrinkage = 0.2, feedforward = TRUE, verbose = TRUE)\n```\n\n```\n## mape[1] = 2.06\n## mape[2] = 1.91\n## mape[3] = 2.09\n## mape[4] = 1.92\n## mape[5] = 2.04\n## mape[6] = 2.02\n```\n\n```\n## [1] 2.01\n```\n\n```r\nmodel_mape(gbm, load ~ . - day, loads.basic, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 4, shrinkage = 0.2, feedforward = TRUE, verbose = TRUE)\n```\n\n```\n## mape[1] = 1.79\n## mape[2] = 1.89\n## mape[3] = 2.22\n## mape[4] = 1.89\n## mape[5] = 1.97\n## mape[6] = 1.87\n```\n\n```\n## [1] 1.94\n```\n\n```r\nmodel_mape(gbm, load ~ . - day, loads.basic, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 5, shrinkage = 0.2, feedforward = TRUE, verbose = TRUE)\n```\n\n```\n## mape[1] = 1.9\n## mape[2] = 1.92\n## mape[3] = 1.95\n## mape[4] = 1.88\n## mape[5] = 1.86\n## mape[6] = 1.82\n```\n\n```\n## [1] 1.89\n```\n\n```r\nmodel_mape(gbm, load ~ . - day, loads.basic, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 6, shrinkage = 0.2, feedforward = TRUE, verbose = TRUE)\n```\n\n```\n## mape[1] = 1.79\n## mape[2] = 1.62\n## mape[3] = 1.91\n## mape[4] = 1.79\n## mape[5] = 1.81\n## mape[6] = 1.99\n```\n\n```\n## [1] 1.82\n```\n\n```r\nmodel_mape(gbm, load ~ . - day, loads.basic, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 7, shrinkage = 0.2, feedforward = TRUE, verbose = TRUE)\n```\n\n```\n## mape[1] = 1.74\n## mape[2] = 1.76\n## mape[3] = 1.84\n## mape[4] = 1.81\n## mape[5] = 2.15\n## mape[6] = 1.73\n```\n\n```\n## [1] 1.84\n```\n\nThe MAPE decreases slightly as we increase the depth.  Beyond 4 there doesn't seem to be significant improvement.\n\nAnd now, the long-term model:\n\n\n```r\nmodel_mape(gbm, load ~ . - day - load.prev, loads.basic, distribution = \"gaussian\", \n    n.trees = 1000, interaction.depth = 3, shrinkage = 0.2, verbose = TRUE)\n```\n\n```\n## mape[1] = 2.31\n## mape[2] = 2.4\n## mape[3] = 2.64\n## mape[4] = 2.27\n## mape[5] = 2.48\n## mape[6] = 2.48\n```\n\n```\n## [1] 2.43\n```\n\n```r\nmodel_mape(gbm, load ~ . - day - load.prev, loads.basic, distribution = \"gaussian\", \n    n.trees = 1000, interaction.depth = 4, shrinkage = 0.2, verbose = TRUE)\n```\n\n```\n## mape[1] = 2.64\n## mape[2] = 2.32\n## mape[3] = 2.24\n## mape[4] = 2.42\n## mape[5] = 2.57\n## mape[6] = 2.29\n```\n\n```\n## [1] 2.42\n```\n\n```r\nmodel_mape(gbm, load ~ . - day - load.prev, loads.basic, distribution = \"gaussian\", \n    n.trees = 1000, interaction.depth = 5, shrinkage = 0.2, verbose = TRUE)\n```\n\n```\n## mape[1] = 2.59\n## mape[2] = 2.47\n## mape[3] = 2.26\n## mape[4] = 2.43\n## mape[5] = 2.62\n## mape[6] = 2.23\n```\n\n```\n## [1] 2.43\n```\n\n```r\nmodel_mape(gbm, load ~ . - day - load.prev, loads.basic, distribution = \"gaussian\", \n    n.trees = 1000, interaction.depth = 6, shrinkage = 0.2, verbose = TRUE)\n```\n\n```\n## mape[1] = 2.39\n## mape[2] = 2.39\n## mape[3] = 2.38\n## mape[4] = 2.69\n## mape[5] = 2.61\n## mape[6] = 2.29\n```\n\n```\n## [1] 2.46\n```\n\n```r\nmodel_mape(gbm, load ~ . - day - load.prev, loads.basic, distribution = \"gaussian\", \n    n.trees = 1000, interaction.depth = 7, shrinkage = 0.2, verbose = TRUE)\n```\n\n```\n## mape[1] = 2.46\n## mape[2] = 2.49\n## mape[3] = 2.35\n## mape[4] = 2.73\n## mape[5] = 2.22\n## mape[6] = 2.48\n```\n\n```\n## [1] 2.46\n```\n\n\nFor the long-term model, a depth of 4 seems to be best.  So we'll use it for both models.\n\n\n#### shrinkage\n\nWith shrinkage, we want the largest value that gives good performance because a larger\nshrinkage might allow us to use less trees.\n\nFirst the short-term model:\n\n```r\nmodel_mape(gbm, load ~ . - day, loads.basic, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 4, shrinkage = 0.1, feedforward = TRUE, verbose = TRUE)\n```\n\n```\n## mape[1] = 2.09\n## mape[2] = 2.07\n## mape[3] = 1.88\n## mape[4] = 2.05\n## mape[5] = 1.9\n## mape[6] = 2.08\n```\n\n```\n## [1] 2.01\n```\n\n```r\nmodel_mape(gbm, load ~ . - day, loads.basic, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 4, shrinkage = 0.15, feedforward = TRUE, verbose = TRUE)\n```\n\n```\n## mape[1] = 2.01\n## mape[2] = 1.84\n## mape[3] = 1.94\n## mape[4] = 1.78\n## mape[5] = 2.15\n## mape[6] = 1.91\n```\n\n```\n## [1] 1.94\n```\n\n```r\nmodel_mape(gbm, load ~ . - day, loads.basic, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 4, shrinkage = 0.2, feedforward = TRUE, verbose = TRUE)\n```\n\n```\n## mape[1] = 1.99\n## mape[2] = 2.09\n## mape[3] = 1.99\n## mape[4] = 1.89\n## mape[5] = 1.93\n## mape[6] = 1.89\n```\n\n```\n## [1] 1.96\n```\n\n```r\nmodel_mape(gbm, load ~ . - day, loads.basic, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 4, shrinkage = 0.25, feedforward = TRUE, verbose = TRUE)\n```\n\n```\n## mape[1] = 1.9\n## mape[2] = 1.75\n## mape[3] = 1.84\n## mape[4] = 2.08\n## mape[5] = 1.8\n## mape[6] = 1.99\n```\n\n```\n## [1] 1.89\n```\n\n```r\nmodel_mape(gbm, load ~ . - day, loads.basic, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 4, shrinkage = 0.3, feedforward = TRUE, verbose = TRUE)\n```\n\n```\n## mape[1] = 1.85\n## mape[2] = 2.05\n## mape[3] = 1.83\n## mape[4] = 1.88\n## mape[5] = 1.9\n## mape[6] = 1.8\n```\n\n```\n## [1] 1.89\n```\n\n\nIt flattens out between 0.2 and 0.3.  Let's see what the long-term model looks like before deciding.\n\nFor the long-term model:\n\n\n```r\nmodel_mape(gbm, load ~ . - day - load.prev, loads.basic, distribution = \"gaussian\", \n    n.trees = 1000, interaction.depth = 4, shrinkage = 0.1, verbose = TRUE)\n```\n\n```\n## mape[1] = 2.42\n## mape[2] = 2.21\n## mape[3] = 2.52\n## mape[4] = 2.51\n## mape[5] = 2.58\n## mape[6] = 2.34\n```\n\n```\n## [1] 2.43\n```\n\n```r\nmodel_mape(gbm, load ~ . - day - load.prev, loads.basic, distribution = \"gaussian\", \n    n.trees = 1000, interaction.depth = 4, shrinkage = 0.15, verbose = TRUE)\n```\n\n```\n## mape[1] = 2.33\n## mape[2] = 2.38\n## mape[3] = 2.65\n## mape[4] = 2.53\n## mape[5] = 2.52\n## mape[6] = 2.25\n```\n\n```\n## [1] 2.44\n```\n\n```r\nmodel_mape(gbm, load ~ . - day - load.prev, loads.basic, distribution = \"gaussian\", \n    n.trees = 1000, interaction.depth = 4, shrinkage = 0.2, verbose = TRUE)\n```\n\n```\n## mape[1] = 2.17\n## mape[2] = 2.62\n## mape[3] = 2.32\n## mape[4] = 2.37\n## mape[5] = 2.45\n## mape[6] = 2.6\n```\n\n```\n## [1] 2.42\n```\n\n```r\nmodel_mape(gbm, load ~ . - day - load.prev, loads.basic, distribution = \"gaussian\", \n    n.trees = 1000, interaction.depth = 4, shrinkage = 0.25, verbose = TRUE)\n```\n\n```\n## mape[1] = 2.13\n## mape[2] = 2.51\n## mape[3] = 2.64\n## mape[4] = 2.54\n## mape[5] = 2.5\n## mape[6] = 2.19\n```\n\n```\n## [1] 2.42\n```\n\n```r\nmodel_mape(gbm, load ~ . - day - load.prev, loads.basic, distribution = \"gaussian\", \n    n.trees = 1000, interaction.depth = 4, shrinkage = 0.3, verbose = TRUE)\n```\n\n```\n## mape[1] = 2.57\n## mape[2] = 2.18\n## mape[3] = 2.6\n## mape[4] = 2.42\n## mape[5] = 2.38\n## mape[6] = 2.61\n```\n\n```\n## [1] 2.46\n```\n\n\nThis is best from 0.15 to 0.25 so the overlap between short and long-term suggests we choose 0.25.\n\n#### n.trees\n\nWe'd like to use as few trees as we can without increasing MAPE.\n\n\n```r\nmodel_mape(gbm, load ~ . - day, loads.basic, distribution = \"gaussian\", n.trees = 1500, \n    interaction.depth = 4, shrinkage = 0.25, feedforward = TRUE, verbose = TRUE)\n```\n\n```\n## mape[1] = 1.83\n## mape[2] = 2\n## mape[3] = 2.06\n## mape[4] = 1.92\n## mape[5] = 1.77\n## mape[6] = 1.88\n```\n\n```\n## [1] 1.91\n```\n\n```r\nmodel_mape(gbm, load ~ . - day, loads.basic, distribution = \"gaussian\", n.trees = 1000, \n    interaction.depth = 4, shrinkage = 0.25, feedforward = TRUE, verbose = TRUE)\n```\n\n```\n## mape[1] = 1.87\n## mape[2] = 1.99\n## mape[3] = 2.05\n## mape[4] = 1.98\n## mape[5] = 1.86\n## mape[6] = 1.86\n```\n\n```\n## [1] 1.94\n```\n\n```r\nmodel_mape(gbm, load ~ . - day, loads.basic, distribution = \"gaussian\", n.trees = 750, \n    interaction.depth = 4, shrinkage = 0.25, feedforward = TRUE, verbose = TRUE)\n```\n\n```\n## mape[1] = 1.94\n## mape[2] = 1.92\n## mape[3] = 1.94\n## mape[4] = 2.08\n## mape[5] = 1.95\n## mape[6] = 1.9\n```\n\n```\n## [1] 1.95\n```\n\n```r\nmodel_mape(gbm, load ~ . - day, loads.basic, distribution = \"gaussian\", n.trees = 500, \n    interaction.depth = 4, shrinkage = 0.25, feedforward = TRUE, verbose = TRUE)\n```\n\n```\n## mape[1] = 1.87\n## mape[2] = 2.18\n## mape[3] = 2.03\n## mape[4] = 1.85\n## mape[5] = 2.06\n## mape[6] = 2.02\n```\n\n```\n## [1] 2\n```\n\nIt looks like the elbow in this curve is around 1000 trees.\n\n\n```r\nmodel_mape(gbm, load ~ . - day - load.prev, loads.basic, distribution = \"gaussian\", \n    n.trees = 1500, interaction.depth = 4, shrinkage = 0.25, verbose = TRUE)\n```\n\n```\n## mape[1] = 2.62\n## mape[2] = 2.44\n## mape[3] = 2.49\n## mape[4] = 2.57\n## mape[5] = 2.22\n## mape[6] = 2.45\n```\n\n```\n## [1] 2.46\n```\n\n```r\nmodel_mape(gbm, load ~ . - day - load.prev, loads.basic, distribution = \"gaussian\", \n    n.trees = 1000, interaction.depth = 4, shrinkage = 0.25, verbose = TRUE)\n```\n\n```\n## mape[1] = 2.25\n## mape[2] = 2.46\n## mape[3] = 2.3\n## mape[4] = 2.44\n## mape[5] = 2.49\n## mape[6] = 2.66\n```\n\n```\n## [1] 2.44\n```\n\n```r\nmodel_mape(gbm, load ~ . - day - load.prev, loads.basic, distribution = \"gaussian\", \n    n.trees = 750, interaction.depth = 4, shrinkage = 0.25, verbose = TRUE)\n```\n\n```\n## mape[1] = 2.71\n## mape[2] = 2.3\n## mape[3] = 2.3\n## mape[4] = 2.55\n## mape[5] = 2.44\n## mape[6] = 2.46\n```\n\n```\n## [1] 2.46\n```\n\n```r\nmodel_mape(gbm, load ~ . - day - load.prev, loads.basic, distribution = \"gaussian\", \n    n.trees = 500, interaction.depth = 4, shrinkage = 0.25, verbose = TRUE)\n```\n\n```\n## mape[1] = 2.38\n## mape[2] = 2.5\n## mape[3] = 2.38\n## mape[4] = 2.39\n## mape[5] = 2.53\n## mape[6] = 2.62\n```\n\n```\n## [1] 2.47\n```\n\n\nIt looks like we could reasonably get away with fewer than 1000 trees for the long-term model.\nBut for consistency, let's stick with 1000.\n\n#### final settings\n\nHere's the settings that seem to work well for short and long-term models\n\n * n.trees = 1000\n * interaction.depth = 4\n * shrinkage = 0.25\n \n## Summary\n\nFor now we will use 2 models to forecast load.  The short-term model will feed forward the\nlast observed load for 12 hours.  After 12 hours we will switch to the long-term model.  Both\nmodels will use the gradient boosting trees algorithm with the settings described in the previous\nsection.\n\n\n\n## Appendix - Generating this Document\n\nIf you are in RStudio and want an HTML version of this document, just click Knit HTML above this pane.  If you want to generate the markdown for the github wiki, keep reading.\n\nFirst, configure the figure path so that the images end up in the images folder with\na relative url in \"analysis/images/\"\n```r\nrequire(knitr)\ngetwd()\nsetwd(\"/your/path/to/advanced-apps.wiki/analysis\")  # change this line to your path\nopts_knit$set(base.dir = '..')\nopts_chunk$set(fig.path = paste0(basename(getwd()), '/images/'))\n```\n\nThen you can generate the markdown for this page using the `knitr` package\n```r\nknit(\"fcu.Rmd\", output=\"Analyzing-FCU-Data-with-R.md\")\n```\n\n","markers":{"markers":{"1":{"id":1,"range":[[0,0],[0,0]],"tailed":false,"reversed":false,"valid":true,"invalidate":"never","persistent":true,"properties":{"type":"selection","editorId":4},"deserializer":"Marker"}},"deserializer":"MarkerManager"},"history":{"undoStack":[],"redoStack":[],"deserializer":"History"},"encoding":"utf8","filePath":"/Users/tra/src/advanced-apps.wiki/analysis/Analyzing-FCU-Data-with-R.md","modifiedWhenLastPersisted":false,"digestWhenLastPersisted":"ee9d2d48a4abf2bf3ef74875ec9fa95733369704","deserializer":"TextBuffer"},{"text":"\n\n\n## Introduction\n\nThe purpose of this document is the analysis of different ways to estimate\nload using a set of data from PJM.\n\nIf you want to run these examples yourself then [download RStudio](http://www.rstudio.com/),\ndownload the [raw version of Rmd file](./analysis/pjm.Rmd) you are currently reading,\nthen click on the lines of R code as you read and hit Command-Return when your cursor is on the\nline you want to execute.\n\nIf you encounter problems on any of the `require` lines then it means you need to install the\npacakge that is being required.  Do that be selecting \"Install Packages...\" from the `Tools`\nmenu and installing the package of the same name.\n\nAlso, if you ever want help on a particular command, like `plot`, you can type `help(plot)` or \nthe shorthand version of that `?plot`.\n\n## The Data\n\nBelow is a description of how the data was obtained for this analysis.  If you want\nto skip the details, you can just [download the final data file here](./analysis/data/pjm/pep-2013.txt).\nAnd, if you are following along in RStudio, you can load this file as follows:\n\n\n```r\ngetwd()  # prints the current directory\n```\n\n```\n## [1] \"/Users/tra/src/advanced-apps.wiki/analysis\"\n```\n\n```r\nsetwd(\"./data/pjm\")  # change this to wherever you downloaded the file\npep = read.csv(\"pep-2013.txt\")\n```\n\n\nThe load data used for this analysis came from [the PJM website](http://pjm.com/markets-and-operations/ops-analysis/historical-load-data.aspx).  Specifically, the data\ncame from the the [2013 hourly load spreadsheet](http://pjm.com/pub/operations/hist-meter-load/2013-hourly-loads.xls).  That file contains all of the PJM regions and this analysis just\nused only the data in the PEP tab of that spreadsheet.  If you're interested, you can get the\n[raw CSV here](./analysis/data/pjm/2013-hourly-loads.csv).\n\nThe weather data came from [NOAA's website](http://www7.ncdc.noaa.gov/CDO/dataproduct) and is\nthe historical data for weather station 13743 in Washington, DC. To get this data, select\n`Surface Data, Hourly Global`, submit.  On the next page, use the Simplified options, then\nselect United States, District of Columbia with `Selected Stations in state`.  Finally,\nyou can choose the weather station for Washington/National (USAF: 13743) then specify the \ndate range of interest (all of 2013 for this analysis). Ultimately you don't get the data\nfile directly, but they email a link to you so you can download it.  If you don't want\nto go to the trouble, [here's the file for 2013](./analysis/data/pjm/2013-weather-dc-national.txt).\nAnd [this document](http://hurricane.ncdc.noaa.gov/cdo/3505doc.txt) describes what the\nvarious fields mean.\n\nThe load and weather data were combined into a single file after a bit of massaging which\nis the file described in the first paragraph of this section.  The \"massaging\" was mostly\nmaking decisions about which weather data points to choose because sometimes the weather file\ncontains multiple records in an hour.  Times were rounded to the nearest hour and the weather\nrecords that had all the fields filled in were chosen.\n\n### Visualization\n\nR has some nice tools for visualizing datasets which is a useful way to sanity check the\ndata.  Let's visualize the data in tabular and plot forms.\n\n\n```r\nView(pep)\nplot(pep)\n```\n\n<img src=\"analysis/images/PEP-data.png\" title=\"plot of chunk PEP-data\" alt=\"plot of chunk PEP-data\" style=\"display: block; margin: auto;\" />\n\n\nIf you have lots of data, the above `plot` command can be slow, so you can randomly\nsample the data to make a sparser version of the same plot that displays faster.\n\n\n```r\nplot(pep[sample(1:nrow(pep), 1000), ])\n```\n\n\nIf you want to look at an individual plot from this data, specify the axes in a formula.\n\n```r\nplot(load ~ temperature, pep)\n```\n\n<img src=\"analysis/images/load-vs-temperature.png\" title=\"plot of chunk load-vs-temperature\" alt=\"plot of chunk load-vs-temperature\" style=\"display: block; margin: auto;\" />\n\nThis particular plot shows a definitive non-linear relationship between load and temperature. \n\n### Transformation\n\nThere are a couple of other plots that have something strange going on.  Let's create 2 plots\nin one by setting the mfrow parameter.\n\n```r\npar(mfrow = c(1, 2))\nplot(load ~ wind_dir, pep)\nplot(load ~ cloud_ceiling, pep)\n```\n\n<img src=\"analysis/images/wind-cloud.png\" title=\"plot of chunk wind-cloud\" alt=\"plot of chunk wind-cloud\" style=\"display: block; margin: auto;\" />\n\nThere appears to be a fair number of outlier points in these plots.\nLooking at the [field descriptions](http://hurricane.ncdc.noaa.gov/cdo/3505doc.txt) for\nthe weather data, a `wind_direction` of 990 indicates that it is variable. \nLikewise a `cloud_ceiling` of 722 means that it is unlimited.\nSince these outliers could affect models, especially linear or polynomial fits,\nlet's remove the points that have these outliers and create a new\ndataset named `pep1` from the original.\n\n```r\npep1 = subset(pep, wind_dir < 990 & cloud_ceiling < 722)\nplot(pep1[sample(1:nrow(pep1), 1000), ])\n```\n\n\nAlso, it seems a little strange to try to fit wind direction and wind speed separately.\nWhat does it mean to fit to a wind direction of 90 degrees if the wind speed is zero?\nSo let's try to combine those components to create `wind_north` and `wind_east` values instead.\n\n```r\npep2 = pep1\n# add the new variables\npep2$wind_north = cos(pep2$wind_dir * pi/180) * pep2$wind_speed\npep2$wind_east = sin(pep2$wind_dir * pi/180) * pep2$wind_speed\n# remove the old variables\npep2$wind_dir = pep2$wind_speed = NULL\nView(pep2)\nplot(pep2[sample(1:nrow(pep2), 1000), ])\n```\n\n<img src=\"analysis/images/PEP2-data.png\" title=\"plot of chunk PEP2-data\" alt=\"plot of chunk PEP2-data\" style=\"display: block; margin: auto;\" />\n\n\n## Models\n\nNow it's time to try modeling the load.  At the beginning of each section is \nthe needed require calls and the appropriate help call that describes the method\nif you are following along in RStudio.\n\nWe'll start with the simple linear regression models and work our way up to the\nmore advanced tree-based regression models and finish with neural networks.\n\n### Linear Models\n\n```r\nhelp(lm)\n```\n\nMost of the packages in R provide a nice formula syntax for fitting a model to the data.\nFor example, if we want a simple model of the load as a linear function of temperature,\n\n```r\nfit = lm(load ~ temperature, pep2)\nsummary(fit)\n```\n\n```\n## \n## Call:\n## lm(formula = load ~ temperature, data = pep2)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1406.0  -488.4   -71.8   472.2  2429.0 \n## \n## Coefficients:\n##             Estimate Std. Error t value            Pr(>|t|)    \n## (Intercept) 3030.677     35.258    86.0 <0.0000000000000002 ***\n## temperature    8.623      0.573    15.1 <0.0000000000000002 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 695 on 5141 degrees of freedom\n## Multiple R-squared:  0.0422,\tAdjusted R-squared:  0.042 \n## F-statistic:  227 on 1 and 5141 DF,  p-value: <0.0000000000000002\n```\n\nMost of the models we will look at have a `summary` and `plot` method that give\nmore information about the model fit.  You can see from this summary that we have\nnot done a good job of fitting the data.  The `R-squared` values of 0.04 indicates\nthat we have only explained 4% of the data variance.\n\n```r\npar(mfrow = c(2, 2))  # prepare for 4 plots at once\nplot(fit)\n```\n\n<img src=\"analysis/images/lm-temp.png\" title=\"plot of chunk lm-temp\" alt=\"plot of chunk lm-temp\" style=\"display: block; margin: auto;\" />\n\nAnd the residuals confirm that we need a better model.\n\nIt's a simple matter to add observations.\n\n```r\nsummary(lm(load ~ yday + hour + temperature, pep2))\n```\n\n```\n## \n## Call:\n## lm(formula = load ~ yday + hour + temperature, data = pep2)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1430.2  -437.0   -75.4   392.2  2223.1 \n## \n## Coefficients:\n##              Estimate Std. Error t value            Pr(>|t|)    \n## (Intercept) 2673.7052    34.2895   77.97 <0.0000000000000002 ***\n## yday          -0.6983     0.0827   -8.44 <0.0000000000000002 ***\n## hour          45.8633     1.2464   36.80 <0.0000000000000002 ***\n## temperature    7.5541     0.5227   14.45 <0.0000000000000002 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 614 on 5139 degrees of freedom\n## Multiple R-squared:  0.252,\tAdjusted R-squared:  0.252 \n## F-statistic:  578 on 3 and 5139 DF,  p-value: <0.0000000000000002\n```\n\nAnd you can add interactions between those terms with the `*` operator.\n\n```r\nsummary(lm(load ~ yday * hour * temperature, pep2))\n```\n\n```\n## \n## Call:\n## lm(formula = load ~ yday * hour * temperature, data = pep2)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1410.6  -433.8   -71.5   387.4  2197.7 \n## \n## Coefficients:\n##                           Estimate   Std. Error t value\n## (Intercept)           3500.6396262  129.3789714   27.06\n## yday                    -2.6379887    0.6570099   -4.02\n## hour                     9.1194149    9.3142236    0.98\n## temperature             -8.7052262    2.7002934   -3.22\n## yday:hour               -0.0160471    0.0473612   -0.34\n## yday:temperature         0.0441292    0.0135255    3.26\n## hour:temperature         0.6346957    0.1880741    3.37\n## yday:hour:temperature    0.0000953    0.0009438    0.10\n##                                   Pr(>|t|)    \n## (Intercept)           < 0.0000000000000002 ***\n## yday                               0.00006 ***\n## hour                               0.32758    \n## temperature                        0.00127 ** \n## yday:hour                          0.73476    \n## yday:temperature                   0.00111 ** \n## hour:temperature                   0.00074 ***\n## yday:hour:temperature              0.91957    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 607 on 5135 degrees of freedom\n## Multiple R-squared:  0.271,\tAdjusted R-squared:  0.27 \n## F-statistic:  272 on 7 and 5135 DF,  p-value: <0.0000000000000002\n```\n\nBut we're still only at 27% of the variance explained.\n\nIt's easy to model the fit against *all* observations in the data with the dot notation.\n\n```r\nsummary(lm(load ~ ., pep2))\n```\n\n```\n## \n## Call:\n## lm(formula = load ~ ., data = pep2)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n##  -1430   -440    -82    388   2405 \n## \n## Coefficients:\n##               Estimate Std. Error t value             Pr(>|t|)    \n## (Intercept)   -160.550   1415.239   -0.11                0.910    \n## yday            -0.761      0.084   -9.06 < 0.0000000000000002 ***\n## hour            48.664      1.313   37.06 < 0.0000000000000002 ***\n## precip         -47.817    220.247   -0.22                0.828    \n## cloud_ceiling    0.484      0.112    4.32    0.000016182655689 ***\n## visibility      -5.433      5.106   -1.06                0.287    \n## temperature     -3.571      1.538   -2.32                0.020 *  \n## dewpoint        11.113      1.441    7.71    0.000000000000015 ***\n## pressure         2.874      1.385    2.08                0.038 *  \n## wind_north      -1.301      1.143   -1.14                0.255    \n## wind_east      -19.312      1.902  -10.15 < 0.0000000000000002 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 606 on 5132 degrees of freedom\n## Multiple R-squared:  0.273,\tAdjusted R-squared:  0.271 \n## F-statistic:  193 on 10 and 5132 DF,  p-value: <0.0000000000000002\n```\n\nOne thing to notice is all the `***` terms in the summary.  The terms that have 3 asterisks\nare considered to be significant predictors in the model.  The terms without asterisks are weak\npredictors.  Interestingly, dewpoint is a stronger term than temperature but this is because\nthey are highly correlated with each other.  If we remove dewpoint then temperature will become\na strong predictor.\n\n```r\nsummary(lm(load ~ . - dewpoint, pep2))\n```\n\n```\n## \n## Call:\n## lm(formula = load ~ . - dewpoint, data = pep2)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1399.5  -438.5   -70.7   391.3  2288.7 \n## \n## Coefficients:\n##                Estimate Std. Error t value             Pr(>|t|)    \n## (Intercept)   1942.6517  1396.5832    1.39              0.16428    \n## yday            -0.6737     0.0837   -8.05    0.000000000000001 ***\n## hour            45.2584     1.2435   36.40 < 0.0000000000000002 ***\n## precip         -46.2306   221.4974   -0.21              0.83468    \n## cloud_ceiling    0.1807     0.1056    1.71              0.08714 .  \n## visibility     -17.3004     4.8957   -3.53              0.00041 ***\n## temperature      7.3444     0.6056   12.13 < 0.0000000000000002 ***\n## pressure         0.8540     1.3672    0.62              0.53226    \n## wind_north      -2.8375     1.1317   -2.51              0.01220 *  \n## wind_east      -15.2906     1.8395   -8.31 < 0.0000000000000002 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 610 on 5133 degrees of freedom\n## Multiple R-squared:  0.264,\tAdjusted R-squared:  0.263 \n## F-statistic:  205 on 9 and 5133 DF,  p-value: <0.0000000000000002\n```\n\n\nWe can also add cross-interactions between all of the terms.\nThe next two formulas are equivalent:\n\n```r\nsummary(lm(load ~ . * ., pep2))\n```\n\n\n```r\nsummary(lm(load ~ .^2, pep2))\n```\n\n```\n## \n## Call:\n## lm(formula = load ~ .^2, data = pep2)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1345.9  -279.8   -11.5   268.4  1459.7 \n## \n## Coefficients:\n##                                Estimate    Std. Error t value\n## (Intercept)                47983.999077   5966.787912    8.04\n## yday                          46.595226      9.197232    5.07\n## hour                        -459.909159    149.349389   -3.08\n## precip                    -92219.664031  35556.552877   -2.59\n## cloud_ceiling                 46.666537     13.198507    3.54\n## visibility                 -2463.799763    571.400470   -4.31\n## temperature                  -67.759244    168.778933   -0.40\n## dewpoint                    -636.490977    154.011338   -4.13\n## pressure                     -40.023552      5.833835   -6.86\n## wind_north                   437.577798    137.270509    3.19\n## wind_east                    579.901975    173.393882    3.34\n## yday:hour                     -0.016772      0.008522   -1.97\n## yday:precip                   -4.943483      2.425464   -2.04\n## yday:cloud_ceiling             0.000714      0.000802    0.89\n## yday:visibility               -0.123290      0.032807   -3.76\n## yday:temperature               0.021232      0.010632    2.00\n## yday:dewpoint                 -0.071185      0.009842   -7.23\n## yday:pressure                 -0.043141      0.008933   -4.83\n## yday:wind_north                0.009777      0.007724    1.27\n## yday:wind_east                 0.039609      0.014087    2.81\n## hour:precip                   52.454293     20.610433    2.55\n## hour:cloud_ceiling             0.021926      0.011967    1.83\n## hour:visibility                1.561469      0.534950    2.92\n## hour:temperature              -0.767848      0.147086   -5.22\n## hour:dewpoint                  1.204874      0.136269    8.84\n## hour:pressure                  0.473293      0.146164    3.24\n## hour:wind_north                0.226771      0.117520    1.93\n## hour:wind_east                -0.704109      0.184573   -3.81\n## precip:cloud_ceiling           2.023682      3.564256    0.57\n## precip:visibility             33.875660     69.286039    0.49\n## precip:temperature           -24.240288     55.628438   -0.44\n## precip:dewpoint              -15.093444     63.413029   -0.24\n## precip:pressure               93.488982     35.019259    2.67\n## precip:wind_north            -26.053944     25.747377   -1.01\n## precip:wind_east               2.953641     38.739453    0.08\n## cloud_ceiling:visibility      -0.353008      0.087635   -4.03\n## cloud_ceiling:temperature      0.077633      0.011658    6.66\n## cloud_ceiling:dewpoint        -0.041815      0.011192   -3.74\n## cloud_ceiling:pressure        -0.045089      0.012824   -3.52\n## cloud_ceiling:wind_north       0.075458      0.010805    6.98\n## cloud_ceiling:wind_east        0.066685      0.018536    3.60\n## visibility:temperature        -2.568789      1.475179   -1.74\n## visibility:dewpoint            3.089262      1.445824    2.14\n## visibility:pressure            2.410952      0.559333    4.31\n## visibility:wind_north          0.279759      0.511242    0.55\n## visibility:wind_east          -2.012343      0.837058   -2.40\n## temperature:dewpoint           1.797504      0.029178   61.61\n## temperature:pressure           0.003692      0.165295    0.02\n## temperature:wind_north         0.032906      0.137548    0.24\n## temperature:wind_east         -0.117227      0.221061   -0.53\n## dewpoint:pressure              0.507212      0.150922    3.36\n## dewpoint:wind_north            0.114352      0.128015    0.89\n## dewpoint:wind_east            -0.520417      0.194098   -2.68\n## pressure:wind_north           -0.448359      0.134193   -3.34\n## pressure:wind_east            -0.534925      0.170274   -3.14\n## wind_north:wind_east           0.048858      0.173181    0.28\n##                                       Pr(>|t|)    \n## (Intercept)                 0.0000000000000011 ***\n## yday                        0.0000004201340129 ***\n## hour                                   0.00209 ** \n## precip                                 0.00952 ** \n## cloud_ceiling                          0.00041 ***\n## visibility                  0.0000164947507273 ***\n## temperature                            0.68809    \n## dewpoint                    0.0000364204186310 ***\n## pressure                    0.0000000000076763 ***\n## wind_north                             0.00144 ** \n## wind_east                              0.00083 ***\n## yday:hour                              0.04912 *  \n## yday:precip                            0.04159 *  \n## yday:cloud_ceiling                     0.37349    \n## yday:visibility                        0.00017 ***\n## yday:temperature                       0.04588 *  \n## yday:dewpoint               0.0000000000005422 ***\n## yday:pressure               0.0000014107161243 ***\n## yday:wind_north                        0.20564    \n## yday:wind_east                         0.00495 ** \n## hour:precip                            0.01096 *  \n## hour:cloud_ceiling                     0.06697 .  \n## hour:visibility                        0.00353 ** \n## hour:temperature            0.0000001856251047 ***\n## hour:dewpoint             < 0.0000000000000002 ***\n## hour:pressure                          0.00121 ** \n## hour:wind_north                        0.05371 .  \n## hour:wind_east                         0.00014 ***\n## precip:cloud_ceiling                   0.57022    \n## precip:visibility                      0.62492    \n## precip:temperature                     0.66303    \n## precip:dewpoint                        0.81188    \n## precip:pressure                        0.00762 ** \n## precip:wind_north                      0.31163    \n## precip:wind_east                       0.93923    \n## cloud_ceiling:visibility    0.0000570361303992 ***\n## cloud_ceiling:temperature   0.0000000000304095 ***\n## cloud_ceiling:dewpoint                 0.00019 ***\n## cloud_ceiling:pressure                 0.00044 ***\n## cloud_ceiling:wind_north    0.0000000000032430 ***\n## cloud_ceiling:wind_east                0.00032 ***\n## visibility:temperature                 0.08168 .  \n## visibility:dewpoint                    0.03267 *  \n## visibility:pressure         0.0000166033200145 ***\n## visibility:wind_north                  0.58426    \n## visibility:wind_east                   0.01625 *  \n## temperature:dewpoint      < 0.0000000000000002 ***\n## temperature:pressure                   0.98218    \n## temperature:wind_north                 0.81094    \n## temperature:wind_east                  0.59593    \n## dewpoint:pressure                      0.00078 ***\n## dewpoint:wind_north                    0.37176    \n## dewpoint:wind_east                     0.00736 ** \n## pressure:wind_north                    0.00084 ***\n## pressure:wind_east                     0.00169 ** \n## wind_north:wind_east                   0.77786    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 402 on 5087 degrees of freedom\n## Multiple R-squared:  0.683,\tAdjusted R-squared:  0.68 \n## F-statistic:  199 on 55 and 5087 DF,  p-value: <0.0000000000000002\n```\n\nNow we are explaining a lot more of the variance, 69%, but we have many weak terms which\ncould mean we are overfitting.\n\nHow about we add 3rd-order interactions?\n\n```r\nsummary(lm(load ~ .^3, pep2))\n```\n\nThe above model has 175 features with lots of weak terms, but it demonstrates how easy it\nis to model complex interactions with R.\n\nPolynomial terms are also easy to add with R.\n\n```r\nsummary(lm(load ~ yday + hour + poly(temperature, 2), pep2))\n```\n\n```\n## \n## Call:\n## lm(formula = load ~ yday + hour + poly(temperature, 2), data = pep2)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1192.7  -327.8   -31.4   287.8  1674.3 \n## \n## Coefficients:\n##                         Estimate Std. Error t value            Pr(>|t|)\n## (Intercept)            3115.2516    16.6374  187.24 <0.0000000000000002\n## yday                     -0.1969     0.0602   -3.27              0.0011\n## hour                     38.7091     0.9062   42.72 <0.0000000000000002\n## poly(temperature, 2)1  8700.8967   458.0194   19.00 <0.0000000000000002\n## poly(temperature, 2)2 30899.1844   450.0448   68.66 <0.0000000000000002\n##                          \n## (Intercept)           ***\n## yday                  ** \n## hour                  ***\n## poly(temperature, 2)1 ***\n## poly(temperature, 2)2 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 444 on 5138 degrees of freedom\n## Multiple R-squared:  0.61,\tAdjusted R-squared:  0.61 \n## F-statistic: 2.01e+03 on 4 and 5138 DF,  p-value: <0.0000000000000002\n```\n\n```r\nsummary(lm(load ~ yday + poly(hour, 5) + poly(temperature, 5), pep2))\n```\n\n```\n## \n## Call:\n## lm(formula = load ~ yday + poly(hour, 5) + poly(temperature, \n##     5), data = pep2)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1023.7  -205.8    19.6   207.8  1252.1 \n## \n## Coefficients:\n##                          Estimate  Std. Error t value             Pr(>|t|)\n## (Intercept)             3587.6471      9.4396  380.06 < 0.0000000000000002\n## yday                      -0.2585      0.0454   -5.69  0.00000001343800831\n## poly(hour, 5)1         18615.6905    342.2048   54.40 < 0.0000000000000002\n## poly(hour, 5)2        -16210.7606    340.6908  -47.58 < 0.0000000000000002\n## poly(hour, 5)3         -8411.6019    335.8174  -25.05 < 0.0000000000000002\n## poly(hour, 5)4          7283.3769    335.0843   21.74 < 0.0000000000000002\n## poly(hour, 5)5         -5285.5189    334.4668  -15.80 < 0.0000000000000002\n## poly(temperature, 5)1  10561.2471    349.1653   30.25 < 0.0000000000000002\n## poly(temperature, 5)2  31447.6753    339.6133   92.60 < 0.0000000000000002\n## poly(temperature, 5)3   4481.7684    341.2210   13.13 < 0.0000000000000002\n## poly(temperature, 5)4  -4855.6275    334.8857  -14.50 < 0.0000000000000002\n## poly(temperature, 5)5  -2718.9391    335.9357   -8.09  0.00000000000000072\n##                          \n## (Intercept)           ***\n## yday                  ***\n## poly(hour, 5)1        ***\n## poly(hour, 5)2        ***\n## poly(hour, 5)3        ***\n## poly(hour, 5)4        ***\n## poly(hour, 5)5        ***\n## poly(temperature, 5)1 ***\n## poly(temperature, 5)2 ***\n## poly(temperature, 5)3 ***\n## poly(temperature, 5)4 ***\n## poly(temperature, 5)5 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 333 on 5131 degrees of freedom\n## Multiple R-squared:  0.78,\tAdjusted R-squared:  0.78 \n## F-statistic: 1.66e+03 on 11 and 5131 DF,  p-value: <0.0000000000000002\n```\n\nThis gets our `R-squared` up to 78%.\n\nAnd you can add interactions between all of the polynomial terms to that.\n\n```r\nsummary(lm(load ~ yday + poly(hour, 5) * poly(temperature, 5), pep2))\n```\n\n```\n## \n## Call:\n## lm(formula = load ~ yday + poly(hour, 5) * poly(temperature, \n##     5), data = pep2)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1163.4  -160.5    20.9   178.4  1193.2 \n## \n## Coefficients:\n##                                          Estimate   Std. Error t value\n## (Intercept)                             3589.3462       8.8438  405.86\n## yday                                      -0.1853       0.0381   -4.86\n## poly(hour, 5)1                         18942.7796     316.6078   59.83\n## poly(hour, 5)2                        -16461.8823     352.3629  -46.72\n## poly(hour, 5)3                         -8200.2345     328.6015  -24.95\n## poly(hour, 5)4                          8125.8229     304.2925   26.70\n## poly(hour, 5)5                         -5971.0406     301.9807  -19.77\n## poly(temperature, 5)1                  10950.7494     528.9222   20.70\n## poly(temperature, 5)2                  33788.7648     635.0259   53.21\n## poly(temperature, 5)3                   4732.7970     640.7867    7.39\n## poly(temperature, 5)4                  -4317.8236     566.4463   -7.62\n## poly(temperature, 5)5                  -3634.3491     430.7550   -8.44\n## poly(hour, 5)1:poly(temperature, 5)1  -80757.0709   26538.5944   -3.04\n## poly(hour, 5)2:poly(temperature, 5)1 -706646.0797   32729.7713  -21.59\n## poly(hour, 5)3:poly(temperature, 5)1 -132061.0665   28397.2774   -4.65\n## poly(hour, 5)4:poly(temperature, 5)1  452938.2354   24628.2037   18.39\n## poly(hour, 5)5:poly(temperature, 5)1  303609.6236   24048.8556   12.62\n## poly(hour, 5)1:poly(temperature, 5)2    -797.7222   30588.0055   -0.03\n## poly(hour, 5)2:poly(temperature, 5)2 -204331.2990   39019.3582   -5.24\n## poly(hour, 5)3:poly(temperature, 5)2    2680.5046   32853.7561    0.08\n## poly(hour, 5)4:poly(temperature, 5)2   25066.7118   28031.8966    0.89\n## poly(hour, 5)5:poly(temperature, 5)2  -30629.9914   26340.1699   -1.16\n## poly(hour, 5)1:poly(temperature, 5)3   85884.1705   33177.8462    2.59\n## poly(hour, 5)2:poly(temperature, 5)3  118793.2993   40135.2494    2.96\n## poly(hour, 5)3:poly(temperature, 5)3   -7345.7708   33623.7912   -0.22\n## poly(hour, 5)4:poly(temperature, 5)3  -92342.1669   28790.5153   -3.21\n## poly(hour, 5)5:poly(temperature, 5)3  -20173.7515   27211.9869   -0.74\n## poly(hour, 5)1:poly(temperature, 5)4   90673.8261   31091.8949    2.92\n## poly(hour, 5)2:poly(temperature, 5)4  164536.7497   36010.9788    4.57\n## poly(hour, 5)3:poly(temperature, 5)4   -3426.2967   30860.3325   -0.11\n## poly(hour, 5)4:poly(temperature, 5)4 -102610.7392   28561.0433   -3.59\n## poly(hour, 5)5:poly(temperature, 5)4  -84015.1489   26209.5409   -3.21\n## poly(hour, 5)1:poly(temperature, 5)5  -36435.7722   27985.1896   -1.30\n## poly(hour, 5)2:poly(temperature, 5)5    4553.6004   28724.0105    0.16\n## poly(hour, 5)3:poly(temperature, 5)5   52128.9496   26659.9104    1.96\n## poly(hour, 5)4:poly(temperature, 5)5   42486.9284   24490.2008    1.73\n## poly(hour, 5)5:poly(temperature, 5)5   -6232.3259   23882.8376   -0.26\n##                                                  Pr(>|t|)    \n## (Intercept)                          < 0.0000000000000002 ***\n## yday                                    0.000001210081401 ***\n## poly(hour, 5)1                       < 0.0000000000000002 ***\n## poly(hour, 5)2                       < 0.0000000000000002 ***\n## poly(hour, 5)3                       < 0.0000000000000002 ***\n## poly(hour, 5)4                       < 0.0000000000000002 ***\n## poly(hour, 5)5                       < 0.0000000000000002 ***\n## poly(temperature, 5)1                < 0.0000000000000002 ***\n## poly(temperature, 5)2                < 0.0000000000000002 ***\n## poly(temperature, 5)3                   0.000000000000176 ***\n## poly(temperature, 5)4                   0.000000000000029 ***\n## poly(temperature, 5)5                < 0.0000000000000002 ***\n## poly(hour, 5)1:poly(temperature, 5)1              0.00235 ** \n## poly(hour, 5)2:poly(temperature, 5)1 < 0.0000000000000002 ***\n## poly(hour, 5)3:poly(temperature, 5)1    0.000003395120947 ***\n## poly(hour, 5)4:poly(temperature, 5)1 < 0.0000000000000002 ***\n## poly(hour, 5)5:poly(temperature, 5)1 < 0.0000000000000002 ***\n## poly(hour, 5)1:poly(temperature, 5)2              0.97919    \n## poly(hour, 5)2:poly(temperature, 5)2    0.000000170062778 ***\n## poly(hour, 5)3:poly(temperature, 5)2              0.93498    \n## poly(hour, 5)4:poly(temperature, 5)2              0.37125    \n## poly(hour, 5)5:poly(temperature, 5)2              0.24494    \n## poly(hour, 5)1:poly(temperature, 5)3              0.00966 ** \n## poly(hour, 5)2:poly(temperature, 5)3              0.00309 ** \n## poly(hour, 5)3:poly(temperature, 5)3              0.82707    \n## poly(hour, 5)4:poly(temperature, 5)3              0.00135 ** \n## poly(hour, 5)5:poly(temperature, 5)3              0.45851    \n## poly(hour, 5)1:poly(temperature, 5)4              0.00356 ** \n## poly(hour, 5)2:poly(temperature, 5)4    0.000005014301502 ***\n## poly(hour, 5)3:poly(temperature, 5)4              0.91160    \n## poly(hour, 5)4:poly(temperature, 5)4              0.00033 ***\n## poly(hour, 5)5:poly(temperature, 5)4              0.00136 ** \n## poly(hour, 5)1:poly(temperature, 5)5              0.19299    \n## poly(hour, 5)2:poly(temperature, 5)5              0.87405    \n## poly(hour, 5)3:poly(temperature, 5)5              0.05060 .  \n## poly(hour, 5)4:poly(temperature, 5)5              0.08283 .  \n## poly(hour, 5)5:poly(temperature, 5)5              0.79414    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 279 on 5106 degrees of freedom\n## Multiple R-squared:  0.847,\tAdjusted R-squared:  0.846 \n## F-statistic:  785 on 36 and 5106 DF,  p-value: <0.0000000000000002\n```\n\nNow we're up to 85% of the variance explained.\n\n### Ridge Regression\n\n```r\nrequire(MASS)\nhelp(lm.ridge)\n```\n\n\n[Ridge regression](https://en.wikipedia.org/wiki/Ridge_regression) is just linear\nregression with L2-norm regularization.\nRegularization is used to minimize the effects of overfitting the data.\nIn it's simplest form, it is just like `lm` and should\ngive the same answer when the regularization parameter, `lambda`, is zero.\nSo the following models are equivalent.\n\n```r\nlm(load ~ yday + poly(hour, 5) * poly(temperature, 5), pep2)\nlm.ridge(load ~ yday + poly(hour, 5) * poly(temperature, 5), pep2, lambda = 0)\n```\n\nWith ridge, it gets more interesting when you give it a range of lambda values.\nThis plot shows how the estimate coefficients change as a function of lambda.\n\n```r\nplot(lm.ridge(load ~ yday + poly(hour, 5) * poly(temperature, 5), pep2, lambda = seq(0, \n    10000, 10)))\n```\n\n<img src=\"analysis/images/lm-ridge-lambda.png\" title=\"plot of chunk lm-ridge-lambda\" alt=\"plot of chunk lm-ridge-lambda\" style=\"display: block; margin: auto;\" />\n\nAnd R also gives you a method for solving for the best value of lambda in the provided\nrange using cross-validation,\n\n```r\nselect(lm.ridge(load ~ yday + poly(hour, 5) * poly(temperature, 5), pep2, lambda = seq(0, \n    100, 0.1)))\n```\n\n```\n## modified HKB estimator is 5.912 \n## modified L-W estimator is 6.191 \n## smallest value of GCV  at 2.9\n```\n\nIn the above example, it found that the value of lambda at 2.9 minimized the cross-validation\nerror.\n\n### Generalized Additive Models\n\n```r\nrequire(gam)\n```\n\n```\n## Loading required package: gam\n## Loaded gam 1.09\n```\n\n```r\nhelp(gam)\n```\n\n\nA [Generalized Additive Model](https://en.wikipedia.org/wiki/Generalized_additive_model) is a \nmodel that uses non-linear relationships on the observations using smoothing splines.\nFor example, modeling load against smoothing splines on temperature, dewpoint, hour, yday,\nand pressure looks like this.\n\n\n```r\ngam1 = gam(load ~ s(temperature) + s(hour) + s(yday) + s(pressure), data = pep2)\nsummary(gam1)\n```\n\n```\n## \n## Call: gam(formula = load ~ s(temperature) + s(hour) + s(yday) + s(pressure), \n##     data = pep2)\n## Deviance Residuals:\n##     Min      1Q  Median      3Q     Max \n## -921.02 -215.58   -3.67  212.02 1349.16 \n## \n## (Dispersion Parameter for gaussian family taken to be 107502)\n## \n##     Null Deviance: 2593208185 on 5142 degrees of freedom\n## Residual Deviance: 551055083 on 5126 degrees of freedom\n## AIC: 74197 \n## \n## Number of Local Scoring Iterations: 5 \n## \n## Anova for Parametric Effects\n##                  Df    Sum Sq   Mean Sq F value               Pr(>F)    \n## s(temperature)    1  87922640  87922640   817.9 < 0.0000000000000002 ***\n## s(hour)           1 372652895 372652895  3466.5 < 0.0000000000000002 ***\n## s(yday)           1   2557603   2557603    23.8    0.000001106190408 ***\n## s(pressure)       1   6625905   6625905    61.6    0.000000000000005 ***\n## Residuals      5126 551055083    107502                                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Anova for Nonparametric Effects\n##                Npar Df Npar F               Pr(F)    \n## (Intercept)                                          \n## s(temperature)       3   2789 <0.0000000000000002 ***\n## s(hour)              3   1159 <0.0000000000000002 ***\n## s(yday)              3    148 <0.0000000000000002 ***\n## s(pressure)          3      4              0.0066 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n```r\npar(mfrow = c(2, 2))\nplot(gam1, se = TRUE, col = \"blue\")\n```\n\n<img src=\"analysis/images/gam1.png\" title=\"plot of chunk gam1\" alt=\"plot of chunk gam1\" style=\"display: block; margin: auto;\" />\n\n\nHere's what it looks like if you remove the spline on pressure.\n\n```r\ngam2 = gam(load ~ s(temperature) + s(hour) + s(yday) + pressure, data = pep2)\nsummary(gam2)\n```\n\n```\n## \n## Call: gam(formula = load ~ s(temperature) + s(hour) + s(yday) + pressure, \n##     data = pep2)\n## Deviance Residuals:\n##     Min      1Q  Median      3Q     Max \n## -961.42 -216.06   -3.68  212.85 1356.52 \n## \n## (Dispersion Parameter for gaussian family taken to be 107702)\n## \n##     Null Deviance: 2593208185 on 5142 degrees of freedom\n## Residual Deviance: 552401090 on 5129 degrees of freedom\n## AIC: 74204 \n## \n## Number of Local Scoring Iterations: 5 \n## \n## Anova for Parametric Effects\n##                  Df    Sum Sq   Mean Sq F value               Pr(>F)    \n## s(temperature)    1  89742597  89742597   833.2 < 0.0000000000000002 ***\n## s(hour)           1 372662014 372662014  3460.1 < 0.0000000000000002 ***\n## s(yday)           1   2778310   2778310    25.8    0.000000392833520 ***\n## pressure          1   6475797   6475797    60.1    0.000000000000011 ***\n## Residuals      5129 552401090    107702                                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Anova for Nonparametric Effects\n##                Npar Df Npar F               Pr(F)    \n## (Intercept)                                          \n## s(temperature)       3   2770 <0.0000000000000002 ***\n## s(hour)              3   1156 <0.0000000000000002 ***\n## s(yday)              3    149 <0.0000000000000002 ***\n## pressure                                             \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n```r\npar(mfrow = c(2, 2))\nplot(gam2, se = TRUE, col = \"blue\")\n```\n\n<img src=\"analysis/images/gam2.png\" title=\"plot of chunk gam2\" alt=\"plot of chunk gam2\" style=\"display: block; margin: auto;\" />\n\n\nThere's a function called `anova` (analysis of variance) that can be used to compare\nmultiple fits to each other.\n\n```r\nanova(gam1, gam2, test = \"Cp\")\n```\n\n```\n## Analysis of Deviance Table\n## \n## Model 1: load ~ s(temperature) + s(hour) + s(yday) + s(pressure)\n## Model 2: load ~ s(temperature) + s(hour) + s(yday) + pressure\n##   Resid. Df Resid. Dev Df Deviance        Cp\n## 1      5126  551055083             554710112\n## 2      5129  552401090 -3 -1346007 555411092\n```\n\nTo get information on what this means you can get help on `anova.gam`\n\n```r\nhelp(anova.gam)\n```\n\nThe Cp value is one way of trying to predict the test error from the training data.\nThus we would generally want to choose the model that has the smaller Cp value.\nThe above result seems to indicate that the models have very similar values for Cp\nso we would probably choose the simpler model.\n\n### Regression Trees\n\n```r\nrequire(tree)\n```\n\n```\n## Loading required package: tree\n```\n\n```r\nhelp(tree)\n```\n\n\nSimple regression trees are useful for looking at what observations are important. \nBut by themselves the aren't great predictors.\n\n\n```r\ntree1 = tree(load ~ ., pep2)\npar(mfrow = c(1, 1))\nplot(tree1)\ntext(tree1)\n```\n\n<img src=\"analysis/images/tree.png\" title=\"plot of chunk tree\" alt=\"plot of chunk tree\" style=\"display: block; margin: auto;\" />\n\nThe plot gives a nice visualization of how the tree works.\nIn this example the tree says that `hour` is the most important first split on the data.\nFor time less than 7:30, it then splits on temperature.\nFor times greather than 7:30 it splits on dewpoint.\nWhile simple regression trees by themselves aren't the best predictors,\nthey are the basis for much better methods such as Random Forests and Boosting.\n\n### Random Forests\n\n```r\nrequire(randomForest)\nhelp(randomForest)\n```\n\n\n[Random Forest](https://simple.wikipedia.org/wiki/Random_forest) is a technique where\nmultiple regression trees are generated using a random subset of the observations at each\nbranch.  The forest of all trees is then used to construct the model which gives a better\nestimate than a single regression tree.\n\nUsing the `randomForest` package is pretty simple.\n\n\n```r\nrf1 = randomForest(load ~ ., pep2)\nrf1\n```\n\n```\n## \n## Call:\n##  randomForest(formula = load ~ ., data = pep2) \n##                Type of random forest: regression\n##                      Number of trees: 500\n## No. of variables tried at each split: 3\n## \n##           Mean of squared residuals: 33302\n##                     % Var explained: 93.4\n```\n\n\nNotice that this model explains 93% of the variance without any tuning which is\nvery good.\n\n\n```r\npar(mfrow = c(1, 1))\nplot(rf1)\n```\n\n<img src=\"analysis/images/random-forest-1b.png\" title=\"plot of chunk random-forest-1b\" alt=\"plot of chunk random-forest-1b\" style=\"display: block; margin: auto;\" />\n\n\nThe default plot shows that the error flattens out above 200 (or sooner).\nSo we can generate a forest with 200  trees and output debug information\nalong the way with this command.\n\n\n```r\nrf2 = randomForest(load ~ ., pep2, ntree = 200, do.trace = 25)\n```\n\n```\n##      |      Out-of-bag   |\n## Tree |      MSE  %Var(y) |\n##   25 | 4.165e+04     8.26 |\n##   50 | 3.744e+04     7.42 |\n##   75 | 3.641e+04     7.22 |\n##  100 | 3.538e+04     7.02 |\n##  125 | 3.504e+04     6.95 |\n##  150 | 3.466e+04     6.87 |\n##  175 | 3.431e+04     6.80 |\n##  200 | 3.408e+04     6.76 |\n```\n\n```r\nrf2\n```\n\n```\n## \n## Call:\n##  randomForest(formula = load ~ ., data = pep2, ntree = 200, do.trace = 25) \n##                Type of random forest: regression\n##                      Number of trees: 200\n## No. of variables tried at each split: 3\n## \n##           Mean of squared residuals: 34079\n##                     % Var explained: 93.24\n```\n\n```r\nplot(rf2)\n```\n\n<img src=\"analysis/images/random-forest-2.png\" title=\"plot of chunk random-forest-2\" alt=\"plot of chunk random-forest-2\" style=\"display: block; margin: auto;\" />\n\n\nAnother nice feature of the `randomForest` package is the `importance` function which\ntells you which observations are the most important factors in the model.\n\n\n```r\nimportance(rf2)\n```\n\n```\n##               IncNodePurity\n## yday              243190588\n## hour              882446238\n## precip              8498355\n## cloud_ceiling      59461733\n## visibility         16621195\n## temperature       686681384\n## dewpoint          451206283\n## pressure           83155159\n## wind_north         64414483\n## wind_east          67748963\n```\n\nThis information could be useful when trying to decide which observations don't need\nto be collected.  In this case, `precip` seems to have a small effect.\n\n### Boosting\n\n```r\nrequire(gbm)\nhelp(gbm)\n```\n\n\n[Boosting](https://en.wikipedia.org/wiki/Boosting_\\(machine_learning\\)) is a cousin to Random\nForest.  Whereas Random Forest generates a large number of fairly complex trees and combines\nthem to reduce the variance, Boosting generates a larger number of simple trees and combines\nthem to reduce bias.\n\n\n```r\ngbm1 = gbm(load ~ ., pep2, distribution = \"gaussian\", n.trees = 1000, shrinkage = 0.2)\nsummary(gbm1)\n```\n\n<img src=\"analysis/images/gbm-1-summary.png\" title=\"plot of chunk gbm-1-summary\" alt=\"plot of chunk gbm-1-summary\" style=\"display: block; margin: auto;\" />\n\n```\n##                         var  rel.inf\n## temperature     temperature 39.33937\n## hour                   hour 35.47154\n## dewpoint           dewpoint 14.95968\n## yday                   yday  5.67437\n## pressure           pressure  1.72354\n## wind_north       wind_north  1.09500\n## wind_east         wind_east  0.92294\n## cloud_ceiling cloud_ceiling  0.57177\n## visibility       visibility  0.18261\n## precip               precip  0.05919\n```\n\nThis summary plot is very similar to the `importance` plot for `randomForest` and shows\nthe relative influence of each observation.\n\n```r\npar(mfrow = c(1, 2))\nplot(gbm1, i = \"temperature\")\nplot(gbm1, i = \"hour\")\n```\n\n<img src=\"analysis/images/gbm-1.png\" title=\"plot of chunk gbm-1\" alt=\"plot of chunk gbm-1\" style=\"display: block; margin: auto;\" />\n\nThis plot shows the 'marginal effect' of the selected variables on the model.\nIt is not smooth because of the nature of splitting of data that happens when constructing\ntrees.\n\nSome of the more interesting settings that can be changed to optimize the\nboosting algorithm are the number of trees (`n.trees`),\nthe size of the trees (`interaction.depth`),\nthe regularization value (`shrinkage`),\nand the percentage of observations considered at each branch (`bag.fraction`).\n\nHere's an example  using all of these settings.\n\n```r\ngbm2 = gbm(load ~ ., pep2, distribution = \"gaussian\", n.trees = 2000, interaction.depth = 5, \n    shrinkage = 0.15, bag.fraction = 0.7)\npar(mfrow = c(1, 1))\nsummary(gbm2)\n```\n\n<img src=\"analysis/images/gbm-2-summary.png\" title=\"plot of chunk gbm-2-summary\" alt=\"plot of chunk gbm-2-summary\" style=\"display: block; margin: auto;\" />\n\n```\n##                         var rel.inf\n## hour                   hour 36.3142\n## temperature     temperature 33.3322\n## dewpoint           dewpoint 17.3000\n## yday                   yday  7.0815\n## pressure           pressure  2.1456\n## cloud_ceiling cloud_ceiling  1.2881\n## wind_north       wind_north  1.2862\n## wind_east         wind_east  0.9907\n## visibility       visibility  0.1576\n## precip               precip  0.1039\n```\n\n\n```r\npar(mfrow = c(2, 2))\nplot(gbm2, i = \"hour\")\nplot(gbm2, i = \"temperature\")\nplot(gbm2, i = \"yday\")\nplot(gbm2, i = \"dewpoint\")\n```\n\n<img src=\"analysis/images/gbm-2.png\" title=\"plot of chunk gbm-2\" alt=\"plot of chunk gbm-2\" style=\"display: block; margin: auto;\" />\n\n\n### Neural Networks\n\n```r\nrequire(neuralnet)\nhelp(neuralnet)\n```\n\n\nNeural networks are more commonly used for classification problems but can also be\nused for regression problems if the data is normalized first.\n\nWithout normalization, here's how you create a simple neural network with 7 hidden layers.\nUnfortunately, you can't use the simple `y ~ .` syntax, you must specify each observation\nyou want to use.\n\n\n```r\nnn = neuralnet(load ~ yday + hour + precip + cloud_ceiling + visibility + temperature + \n    dewpoint + pressure + wind_north + wind_east, data = pep2, hidden = c(7))\nplot(nn)\n```\n\n\nThe `hidden` options controls how many layers are built and how many node there are on each layer. \nFor example, if you want 2 hidden layers with 7 and 5 nodes respectively then,\n\n```r\nnn = neuralnet(load ~ yday + hour + precip + cloud_ceiling + visibility + temperature + \n    dewpoint + pressure + wind_north + wind_east, data = pep2, hidden = c(7, \n    5))\nplot(nn)\n```\n\n\nAs mentioned above, this does not product a good prediction because the data hasn't been normalized.\nLet's pick out day 185 and put it in a form that `neuralnet` understand so we can get a prediction.\n\n```r\n\nday185 = pep2[pep2$yday == 185, ]\nday185m = model.matrix(load ~ . - 1, day185)\n\nres = compute(nn, day185m)\nres$net.result[, 1]\n```\n\n```\n##        4274        4275        4276        4277        4280        4281 \n## 3540.899479 3540.899479 3540.899479 3540.899479 3540.899479 3540.899479 \n##        4282        4283        4284        4285        4286        4287 \n## 3540.899479 3540.899479 3540.899479 3540.899479 3540.899479 3540.899479 \n##        4288        4289        4290        4291        4292        4293 \n## 3540.899479 3540.899479 3540.899479 3540.899479 3540.899479 3540.899479 \n##        4294 \n## 3540.899479\n```\n\nAs you can see, it generated the same estimate for every sample which is not good.\n\nHere's one way of normalizing this data.  Start by putting every data point between 0 and 1.\n\n```r\n\n# normalize data between 0 and 1\npep2n = sweep(pep2, 2, apply(pep2, 2, min), \"-\")\npep2n = sweep(pep2n, 2, apply(pep2n, 2, max), \"/\")\nView(pep2n)\n```\n\n\nUsing the default settings for the `neuralnet` package tend to not converge on this data set\nso try increasing the threshold to 0.3.  Also, setting the `lifesign` parameters allows you\nto see how the learning process is doing.\n\n\n```r\nnn = neuralnet(load ~ yday + hour + precip + cloud_ceiling + visibility + temperature + \n    dewpoint + pressure + wind_north + wind_east, data = pep2, lifesign = \"full\", \n    lifesign.step = 5000, threshold = 0.3, hidden = c(7))\n```\n\n```\n## hidden: 7    thresh: 0.3    rep: 1/1    steps:    5000\tmin thresh: 206300.8478\n##                                                   5082\terror: 1296604092 \ttime: 38.89 secs\n```\n\n```r\nplot(nn)\n```\n\nNow normalize the day 185 data to get the normalized result.\n\n```r\nday185n = pep2n[which(pep2$yday == 185), ]\nday185nm = model.matrix(load ~ . - 1, day185n)\nres = compute(nn, day185nm)\nres$net.result[, 1]\n```\n\n```\n##        4274        4275        4276        4277        4280        4281 \n## 1935.903981 1988.129447 2044.701972 2076.022562 1980.232871 1878.699714 \n##        4282        4283        4284        4285        4286        4287 \n## 1893.594661 1896.489397 1911.776193 1930.230024 1931.277257 1961.190579 \n##        4288        4289        4290        4291        4292        4293 \n## 1983.160049 1942.797893 1998.829873 1919.803975 1929.922810 1968.556369 \n##        4294 \n## 1948.044681\n```\n\n\nAnd denormalize the result.\n```\npredictions = res$net.result[,1]*(max(pep2$load)-min(pep2$load))+min(pep2$load)\npredictions\nday185$load\n```\n\nYou can see that getting a good estimate out of the `neuralnet` package with R is difficult.\nIn the next section we will add a helper functions to make this easier.\n\n## Comparisons\n\nYou may have noticed that every package has a different way of reporting results.\nWe would like a model-independent way of comparing each method so we can decide which\nmodel works best for the data.\n\nThe following method does a K-Fold cross-validation for a given model in a generic way.\nA K-Fold works by splitting the data into K pieces, training each combination of K-1 pieces\nand testing against the piece not used in training.  That process is repeated K times and\naveraged to compute the overall mean absolute percentage error (MAPE) for the test data segments.\nBy default this cross-validation splits data on the \"yday\" field in order to avoid using data\npoints that are close together in time (and therefore highly correlated) for both training and prediction.\n\nThis is a good way to compare results across algorithms and gives us apples-to-apples\ncomparisons for how well we expect a given algorithm to predict load.\n\nEvaluate the entire block of code below.\n\n\n```r\n# test model error using K-folds\nmodel_mape = function(model, formula, data, foldon = \"yday\", K = 6, normalize = FALSE, \n    verbose = FALSE, ...) {\n    uniques = unique(data[, foldon])\n    folds = split(sample(uniques), 1:length(uniques)%%K)\n    sape = 0\n    y = all.vars(formula)[1]  # the name of the thing we are solving for\n    \n    # normalize data between 0 and 1\n    if (normalize) {\n        ndata = sweep(data, 2, apply(data, 2, min), \"-\")\n        ndata = sweep(ndata, 2, apply(ndata, 2, max), \"/\")\n        min_y = min(data[y])\n        scale_y = max(data[y]) - min_y\n    } else {\n        ndata = data\n        min_y = 0\n        scale_y = 1\n    }\n    \n    for (fold in 1:K) {\n        test_rows = data[, foldon] %in% unlist(folds[fold])\n        train_data = subset(ndata, !test_rows)\n        test_data = subset(ndata, test_rows)\n        test_data_orig = subset(data, test_rows)\n        \n        fit = model(formula, data = train_data, ...)\n        predictions = predict(fit, test_data, ...)\n        norm_residuals = (predictions - test_data[y])\n        # denormalize\n        residuals = norm_residuals * scale_y\n        \n        # sum of absolute percentage errors for fold K\n        sape_k = sum(abs(residuals/test_data_orig[y]))\n        sape = sape + sape_k\n        if (verbose) {\n            cat(\"mape[\", fold, \"] = \", round(100 * sape_k/nrow(test_data[y]), \n                1), \"\\n\", sep = \"\")\n        }\n    }\n    # mean absolute percentage error (MAPE)\n    round(100 * sape/nrow(data), 1)\n}\n```\n\nAnd since `neuralnet` doesn't define a predict method, we need to write one.\nThis will get called when `predict` is called with the result of a `neuralnet` call.\nSimply put, this gives us an answer that is similar in format to all of the other\n`predict` calls so that we can use it in our model comparisons below.\n\n```r\npredict.nn = function(fit, data, ...) {\n    orig_formula = paste(fit$model.list$response, paste(fit$model.list$variables, \n        collapse = \"+\"), sep = \"~\")\n    # remove the intercept from the original formula since compute doesn't want\n    # that\n    matrix_formula = as.formula(paste(orig_formula, \"- 1\"))\n    datam = model.matrix(matrix_formula, data)\n    result = compute(fit, datam)\n    # return only the last column which is the prediction values\n    predictions = result$net.result[, 1]\n}\n```\n\n\nNow let's calculate MAPE for all of the models.\n\n\n```r\n# linear models\nmodel_mape(lm, load ~ ., pep2)\n```\n\n```\n## [1] 14.2\n```\n\n```r\nmodel_mape(lm, load ~ . * ., pep2)\n```\n\n```\n## [1] 9.6\n```\n\n```r\nmodel_mape(lm, load ~ .^2 + poly(temperature, 5) * poly(hour, 5) - temperature - \n    hour - temperature:hour, pep2)\n```\n\n```\n## [1] 6\n```\n\n\n```r\n# generalized additive models\nmodel_mape(gam, load ~ s(temperature) * s(hour) + s(dewpoint) + s(yday) + s(pressure), \n    pep2)\n```\n\n```\n## [1] 7\n```\n\n\n```r\n# regression tree\nmodel_mape(tree, load ~ ., pep2)\n```\n\n```\n## [1] 8.9\n```\n\n\n```r\n# random forest\nmodel_mape(randomForest, load ~ ., pep2, ntree = 50)\n```\n\n```\n## [1] 5.9\n```\n\n```r\nmodel_mape(randomForest, load ~ ., pep2, ntree = 50, mtry = 6)\n```\n\n```\n## [1] 5.5\n```\n\n```r\nmodel_mape(randomForest, load ~ ., pep2, ntree = 100, mtry = 6)\n```\n\n```\n## [1] 5.5\n```\n\n```r\nmodel_mape(randomForest, load ~ ., pep2, ntree = 200, mtry = 6)\n```\n\n```\n## [1] 5.5\n```\n\n\n```r\n# generalized boosting\nmodel_mape(gbm, load ~ ., pep2, distribution = \"gaussian\", n.trees = 2000, interaction.depth = 4, \n    shrinkage = 0.2)\n```\n\n```\n## [1] 5.8\n```\n\n```r\nmodel_mape(gbm, load ~ ., pep2, distribution = \"gaussian\", n.trees = 2000, interaction.depth = 4, \n    shrinkage = 0.1)\n```\n\n```\n## [1] 5.4\n```\n\n```r\nmodel_mape(gbm, load ~ ., pep2, distribution = \"gaussian\", n.trees = 2000, interaction.depth = 4, \n    shrinkage = 0.15)\n```\n\n```\n## [1] 5.6\n```\n\n```r\nmodel_mape(gbm, load ~ ., pep2, distribution = \"gaussian\", n.trees = 2000, interaction.depth = 5, \n    shrinkage = 0.15)\n```\n\n```\n## [1] 5.6\n```\n\n```r\nmodel_mape(gbm, load ~ ., pep2, distribution = \"gaussian\", n.trees = 2000, interaction.depth = 5, \n    shrinkage = 0.15, bag.fraction = 0.7)\n```\n\n```\n## [1] 5.6\n```\n\n```r\nmodel_mape(gbm, load ~ ., pep2, distribution = \"gaussian\", n.trees = 2000, interaction.depth = 5, \n    shrinkage = 0.15, bag.fraction = 1)\n```\n\n```\n## [1] 5.4\n```\n\n```r\nmodel_mape(gbm, load ~ ., pep2, distribution = \"gaussian\", n.trees = 5000, interaction.depth = 5, \n    shrinkage = 0.15, bag.fraction = 1)\n```\n\n```\n## [1] 5.4\n```\n\n```r\nmodel_mape(gbm, load ~ ., pep2, distribution = \"laplace\", n.trees = 5000, interaction.depth = 5, \n    shrinkage = 0.15, bag.fraction = 1)\n```\n\n```\n## [1] 5.3\n```\n\n\n```r\n# neural networks\nmodel_mape(neuralnet, load ~ yday + hour + precip + cloud_ceiling + visibility + \n    temperature + dewpoint + pressure + wind_north + wind_east, data = pep2, \n    normalize = TRUE, threshold = 0.3, hidden = c(7))\n```\n\n```\n## [1] 6.4\n```\n\n```r\n\nmodel_mape(neuralnet, load ~ yday + hour + precip + cloud_ceiling + visibility + \n    temperature + dewpoint + pressure + wind_north + wind_east, data = pep2, \n    normalize = TRUE, threshold = 0.3, hidden = c(9))\n```\n\n```\n## [1] 6.3\n```\n\n```r\n\nmodel_mape(neuralnet, load ~ yday + hour + precip + cloud_ceiling + visibility + \n    temperature + dewpoint + pressure + wind_north + wind_east, data = pep2, \n    normalize = TRUE, threshold = 0.3, hidden = c(11))\n```\n\n```\n## [1] 6.1\n```\n\n```r\n\nmodel_mape(neuralnet, load ~ yday + hour + precip + cloud_ceiling + visibility + \n    temperature + dewpoint + pressure + wind_north + wind_east, data = pep2, \n    normalize = TRUE, threshold = 0.3, hidden = c(6, 5))\n```\n\n```\n## [1] 6.3\n```\n\n```r\n\nmodel_mape(neuralnet, load ~ yday + hour + precip + cloud_ceiling + visibility + \n    temperature + dewpoint + pressure + wind_north + wind_east, data = pep2, \n    normalize = TRUE, threshold = 0.3, hidden = c(7, 2))\n```\n\n```\n## [1] 6.2\n```\n\n```r\n\nmodel_mape(neuralnet, load ~ yday + hour + precip + cloud_ceiling + visibility + \n    temperature + dewpoint + pressure + wind_north + wind_east, data = pep2, \n    normalize = TRUE, threshold = 0.1, hidden = c(9))\n```\n\n```\n## [1] 5.8\n```\n\n\nAll of the above results were run on the \"transformed\" dataset.  What happens if we run\nsome of them against the original unmodified data?\n\n```r\nmodel_mape(randomForest, load ~ ., pep, ntree = 200, mtry = 6)\n```\n\n```\n## [1] 5.4\n```\n\n```r\n\nmodel_mape(gbm, load ~ ., pep, distribution = \"laplace\", n.trees = 5000, interaction.depth = 5, \n    shrinkage = 0.15, bag.fraction = 1)\n```\n\n```\n## [1] 5.2\n```\n\n```r\n\nmodel_mape(neuralnet, load ~ yday + hour + precip + cloud_ceiling + visibility + \n    temperature + dewpoint + pressure + wind_speed + wind_dir, data = pep, normalize = TRUE, \n    threshold = 0.3, hidden = c(9))\n```\n\n```\n## [1] 6.9\n```\n\n\n## Conclusions\n\nBoosting and Random Forest techniques seem to be the current leaders in prediction accuracy.\nRandom Forest is one of the easiest algorithms to use in terms of working without much tuning.\nBoosting requires a little tuning to figure out the best regularization lambda and tree size,\nbut that effort pays off with the best accuracy.\n\nNeural networks don't appear be as good as either\nBoosting or Random Forest and requires a lot more tuning than either of those algorithms.\n\nAmong Boosting, Random Forest, and Neural Networks, it seems like Boosting is the fastest,\nfollowed by Random Forest, and Neural Networks.  None of them are particularly fast however.\n\nOverall, Boosting seems to be the best algorithm based on prediction accuracy and\nspeed of learning.\n\n## Appendix - Generating this Document\n\nIf you are in RStudio and want an HTML version of this document, just click Knit HTML above this pane.  If you want to generate the markdown for the github wiki, keep reading.\n\nFirst, configure the figure path so that the images end up in the images folder with\na relative url in \"analysis/images/\"\n```r\nrequire(knitr)\nsetwd(\"/your/path/to/advanced-apps.wiki/analysis\")  # copy this line and set to your path\nopts_knit$set(base.dir = '..')\nopts_chunk$set(fig.path = paste0(basename(getwd()), '/images/'))\n```\n\nThen you can generate the markdown for this page using the `knitr` package\n```r\nknit(\"pjm.Rmd\", output=\"Analyzing-PJM-Data-with-R.md\")\n```\n","markers":{"markers":{"1":{"id":1,"range":[[0,0],[0,0]],"tailed":false,"reversed":false,"valid":true,"invalidate":"never","persistent":true,"properties":{"type":"selection","editorId":8},"deserializer":"Marker"}},"deserializer":"MarkerManager"},"history":{"undoStack":[],"redoStack":[],"deserializer":"History"},"encoding":"utf8","filePath":"/Users/tra/src/advanced-apps.wiki/analysis/Analyzing-PJM-Data-with-R.md","modifiedWhenLastPersisted":false,"digestWhenLastPersisted":"82d7a06044bd399fc15fcc9e30cd508eda83f426","deserializer":"TextBuffer"},{"text":"```{r, echo=FALSE}\n# this is hidden on the output, it just sets some defaults\nopts_chunk$set(fig.align='center')\nopts_chunk$set(fig.width=5, fig.height=4.5)\nopts_chunk$set(dpi = 110)\nopts_chunk$set(warning = FALSE)\nopts_chunk$set(collapse = TRUE)\npar(mar=c(5.1,4.1,2.1,2.1))\n```\n\n# Load Data\n\nFirst download the Fort Collins load data from [this zip file](./analysis/data/ft-collins-history-2011-2012.tar.gz) and unpack it somewhere.  Then set your working directory to the directory where\nyou unpacked the file (above the 'csvs' directory).\n\nThe following block of code will load up all of the CSV files and pull the hour, day-of-year,\nand day-of-week information into their own columns.\n\n```{r load-data}\n# set working directory to wherever you have unpacked ft-collins-history-2011-2012.tar.gz\nsetwd('~/src/advanced-apps.wiki/analysis/data/fcu')\n\nloads = NULL\nfor (year in 2011:2013) {\n  for (month in 1:12) {\n    filename = paste0('./csvs/', month, '-', year, '.csv')\n    cat(\"loading \", filename, \"\\n\")\n    data = read.csv(filename)\n    # convert garbage characters to integer\n    if (is.factor(data$value)) {\n      data$value = as.integer(as.character(data$value))\n    }\n    # get rid of stuff that looks bad\n    data = data[!is.na(data$value) & data$value>0,]\n    \n    # ignore minutes:seconds-tz part, truncate to the hour, then add hour to get to hour-ending time\n    # also, keep times in MST to avoid hour shift at DST boundaries\n    # (considered using UTC but that would put peaks around 0300 which will probably be bad)\n    stimes = gsub('(T..):..:..(-..):(..)$', '\\\\1\\\\2\\\\3', data$collectedAt)\n    hours = strptime(stimes, \"%Y-%m-%dT%H%z\", tz=\"MST\")+3600\n    loads.month = aggregate(data$value, list(hours), mean)\n    loads = rbind(loads, loads.month)\n    rm(stimes); rm(hours) ; rm(data) ; rm(loads.month)\n  }\n}\n\n# do this after looping over the data\ncolnames(loads) = c(\"time\", \"load\")\nloads$hour = as.POSIXlt(loads$time)$hour\nloads$wday = ((as.POSIXlt(loads$time)$wday + 6) %% 7) + 1  # put sunday at 7 so it's next to saturday\nloads$yday = as.POSIXlt(loads$time)$yday\nloads$day = 365*(as.POSIXlt(loads$time)$year - 111) + as.POSIXlt(loads$time)$yday\n```\n\nBefore removing the `time` field, save a copy for later (when we merge weather).\n```{r loads-orig}\nloads.orig = loads\nloads$time = NULL\n```\n\n## MAPE Function\n\nFort Collins wants to measure their MAPE from hours ending 11:00 to 21:00.  The following function\nruns the specified model and outputs the MAPE in that time range.\n```{r model-mape}\n# test model error using K-folds\nmodel_mape = function(model, formula, data, tdata=NULL, foldon=\"day\", K=6, normalize=FALSE, verbose=FALSE, weights=NULL, feedforward=FALSE, ...) {\n  uniques = unique(data[,foldon])\n  folds = split(sample(uniques), 1:length(uniques)%%K)\n  sape = 0\n  y = all.vars(formula)[1]  # the name of the thing we are solving for\n  \n  if (is.null(tdata)) {\n    tdata = data\n  }\n  \n  # normalize data between 0 and 1\n  if (normalize) {\n    ndata = sweep(data, 2, apply(data, 2, min), \"-\")\n    ndata = sweep(ndata, 2, apply(ndata, 2, max), \"/\")\n    ntdata = sweep(tdata, 2, apply(data, 2, min), \"-\")\n    ntdata = sweep(ntdata, 2, apply(ndata, 2, max), \"/\")\n    min_y = min(tdata[y])\n    scale_y = max(tdata[y]) - min_y    \n  } else {\n    ndata = data\n    ntdata = tdata\n    min_y = 0\n    scale_y = 1\n  }\n  min_hour = 11\n  max_hour = 21\n  \n  total_test_count = 0\n  for (fold in 1:K) {\n    test_rows = tdata[,foldon] %in% unlist(folds[fold])\n    train_rows = !(data[,foldon] %in% unlist(folds[fold]))\n\n    train_data = subset(ndata, train_rows)\n    test_data = subset(ntdata, test_rows & ntdata$hour >= min_hour & ntdata$hour <= max_hour)\n    test_data_orig = subset(tdata, test_rows & tdata$hour >= min_hour & tdata$hour <= max_hour)\n\n    if (is.vector(weights)) {\n      # for some reason, weights need to be assigned in a global environment so they're accessible\n      assign('model_mape.weights', subset(weights, !test_rows), inherits=TRUE)\n      fit = model(formula, data=train_data, weights=model_mape.weights, ...)\n      rm('model_mape.weights', inherits=TRUE)\n    } else {\n      fit = model(formula, data=train_data, ...)\n    }\n    if (feedforward) {\n      for (cur_hour in min_hour:max_hour) {\n        test_hour = subset(test_data, test_data$hour == cur_hour)\n        if (cur_hour > min_hour) {\n#          test_hour$load.prev = apply(test_hour[,c('day', 'hour')], 1, function(x) test_data[test_data$day==x['day']&test_data$hour==(cur_hour-1),]$prediction[1])\n          test_hour$load.prev = apply(test_hour[,c('day', 'hour')], 1, function(x) test_data[test_data$day==x['day']&test_data$hour==(cur_hour-1),]$prediction[1])\n        }\n        cur_predictions = predict(fit, test_hour, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.2, verbose=TRUE)\n        # put the predictions on test_data so that we can get them in the correct order later\n        test_data$prediction[test_data$hour == cur_hour] = cur_predictions\n      }\n      predictions = test_data$prediction\n    } else {\n      predictions = predict(fit, test_data, ...)\n    }\n    norm_residuals = (predictions - test_data[y])\n    # denormalize\n    residuals = norm_residuals*scale_y\n\n    # sum of absolute percentage errors for fold K\n    sape_k = sum(abs(residuals / test_data_orig[y]))\n    sape = sape + sape_k\n    test_count = nrow(test_data[y])\n    total_test_count = total_test_count + test_count\n    if (verbose) {\n      cat(\"mape[\", fold, \"] = \", round(100*sape_k/test_count,2), \"\\n\", sep=\"\")\n    }\n  }\n  # mean absolute percentage error (MAPE)\n  round(100*sape/total_test_count, 2)\n}\n```\nAnd since `neuralnet` doesn't define a predict method, we need to write one.\nThis will get called when `predict` is called with the result of a `neuralnet` call.\nSimply put, this gives us an answer that is similar in format to all of the other\n`predict` calls so that we can use it in our model comparisons below.\n```{r predict_nn}\npredict.nn = function(fit, data, ...) {\n  orig_formula = paste(fit$model.list$response, paste(fit$model.list$variables, collapse=\"+\"), sep=\"~\")\n  # remove the intercept from the original formula since compute doesn't want that\n  matrix_formula = as.formula(paste(orig_formula, \"- 1\"))\n  datam = model.matrix(matrix_formula, data)\n  result = compute(fit, datam)\n  # return only the last column which is the prediction values\n  predictions = result$net.result[,1]\n}\n```\n\n## Models without Weather\n\nFirst, to get a baseline of our worst models, let's try some simple fits to the data.\n\n```{r gbm-simple}\nrequire(gbm)\nfit = gbm(load~., loads, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.2)\nsummary(fit)\n```\n```{r gbm-simple-plot, echo=FALSE, fig.width=7}\npar(mfrow=c(1,3))\nplot(fit, i=\"hour\")\nplot(fit, i=\"yday\")\nplot(fit, i=\"wday\")\n```\n```{r gbm-simple-mape}\nmodel_mape(gbm, load~., loads, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.2)\n```\n\nSince we are trying to get the best estimates during peak load, what happens if we weight by\nthe load itself?\n```{r gbm-weighted}\nmodel_mape(gbm, load~., loads, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.2, weights=loads$load)\n```\nNo improvement.  Let's subtract out the minimum and square the result so that the maximum weights have more effect.\n```{r gbm-weighted-more}\nmodel_mape(gbm, load~., loads, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.2, weights=(loads$load-min(loads$load))^2)\n```\nIt looks like the weights have no discernible effect for `gbm`.\n\n### Adding Yesterday's Load\n\nMany load forecast algorithms use the load from the previous day.\nLet's add that to our feature vector to see how it changes the results.\n\n```{r load-yesterday}\nloads$dayago = apply(loads, 1, function(x) loads[loads$hour==x['hour']&loads$day==(x['day']-1),]$load[1])\n```\n\n```{r yesterday-gbm}\nfit = gbm(load~., loads, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.2)\npar((mfrow=c(1,1)))\nsummary(fit)\n```\nNot surprisingly, the load from the previous day dominates the estimate.  Let's look at MAPE.\n\n```{r yesterday-mape}\nmodel_mape(gbm, load~., loads, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.2)\n# 4.2\n```\n\nWhich is worse than before. Let's try adding week-ago loads too:\n```{r lastweek-gbm}\nloads$weekago = apply(loads, 1, function(x) loads[loads$hour==x['hour']&loads$day==(x['day']-7),]$load[1])\nfit = gbm(load~., loads, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.2)\nsummary(fit)\n```\n\n```{r lastweek-mape}\nmodel_mape(gbm, load~., loads, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.2)\n```\n\nAbout the same. Let's try adding hour-ago loads too:\n```{r hourago-gbm}\nloads$hourago = apply(loads, 1, function(x) loads[loads$hour==(x['hour']-1)&loads$day==x['day'],]$load[1])\nfit = gbm(load~., loads, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.2)\nsummary(fit)\n```\n\n```{r hourago-mape}\nmodel_mape(gbm, load~., loads, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.2)\n```\nThat's much better.  For comparison, how does random forests do with this data?\n\nFirst, the `randomForest` package doesn't deal with `NA` data so let's remove those records.\n```{r loads-without-na, cache=TRUE}\nloads.clean = loads[!is.na(loads$dayago)&!is.na(loads$weekago)&!is.na(loads$hourago),]\n```\n\nAfter trying several settings, the following model is about as good as `randomForest`\ndoes on the data.\nAny number of trees above 80 doesn't appear to improve the results appreciably for this data.\n\n```{r rf-hourago-mape}\nrequire(randomForest)\nmodel_mape(randomForest, load~., loads.clean, ntree=80, mtry=5, verbose=FALSE)\n```\nThis looks a little worse than boosting and it trains noticeably slower.\n\nBut we might not always have the load from an hour ago.  Can we use the\nprevious hour's estimate to feed forward?  We'll come back to that.\nFirst let's load up some weather data and see how that affects the model.\nFor now, remove hour-ago from the data.\n```{r yesterday-gone}\nloads$hourago = NULL\n```\n\n# Weather Data\n\n```{r weather-data}\n# set working directory to wherever the weather csv files are\nsetwd('~/src/advanced-apps.wiki/analysis/data/fcu')\n\nweathers = NULL\nfor (year in 2011:2013) {\n  filename = paste0('./', year, '_ft_collins.csv')\n  cat(\"loading \", filename, \"\\n\")\n  data = read.csv(filename)\n  \n  numeric_fields = c('temperature', 'dew_point', 'wind_speed', 'wind_direction', 'cloud_ceiling', 'visibility', 'pressure')\n  for (field in numeric_fields) {\n    data[field] = as.numeric(as.character(data[field][,1]))\n  }\n\n  # ignore minutes:seconds-tz part, truncate to the hour, then add hour to get to hour-ending time\n  # also, keep times in MST to avoid hour shift at DST boundaries\n  stimes = gsub('(T..):..:..([+-]....)$', '\\\\1\\\\2', data$timestamp)\n  hours = strptime(stimes, \"%Y-%m-%dT%H%z\", tz=\"MST\")+3600\n  weathers.hourly = aggregate(data[,numeric_fields], list(hours), FUN=mean)\n\n  # take the median sky_cover value\n  weathers.hourly$sky_cover = as.factor(levels(data$sky_cover)[aggregate(as.integer(data$sky_cover), list(hours), median)[,2]])\n  \n  weathers = rbind(weathers, weathers.hourly)\n  rm(stimes); rm(hours) ; rm(data) ; rm(weathers.hourly)\n}\ncolnames(weathers)[1] = \"time\"\n```\n\nNow merge the `loads` and `weathers` data frames into one big `data.frame` called `features`.\n```{r merge-loads-weathers}\nloads = merge(loads.orig, weathers, by=\"time\")\nloads$time = NULL\n```\n\n## Models with Weather\n\nLet's get the same summary information for a boosting model that we got without weather:\n```{r gbm-simple-with-weather}\nfit = gbm(load ~ ., loads, distribution = \"gaussian\", n.trees = 1000, interaction.depth = 4, \n    shrinkage = 0.2)\npar((mfrow=c(1,1)))\nsummary(fit)\n```\n\nAs expected temperature plays a large part in the model.  Plotting the top 3 influences:\n```{r gbm-simple-plot-with-weather, echo=FALSE, fig.width=7}\npar(mfrow=c(1,3))\nplot(fit, i=\"temperature\")\nplot(fit, i=\"hour\")\nplot(fit, i=\"yday\")\n```\n\nAnd the MAPE calculation:\n\n```{r gbm-simple-mape-with-weather}\nmodel_mape(gbm, load~., loads, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.2, verbose=TRUE)\n```\nCompared to the same model without weather, we decreased from 3.7% to 2.2% just by adding weather.\n\nNow let's look at what happens if we add loads from previous times at various time periods.\n\n### Previous Load\n\nLet's try adding yesterday's load back in.\n\n```{r load-yesterday-with-weather}\nloads$dayago = apply(loads[,c('load', 'hour', 'day')], 1, function(x) loads[loads$hour==x['hour']&loads$day==(x['day']-1),]$load[1])\n```\n\n\n```{r yesterday-gbm-with-weather}\nfit = gbm(load~., loads, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.2)\npar((mfrow=c(1,1)))\nsummary(fit)\n```\n\n\nLet's look at MAPE.\n\n```{r yesterday-mape-with-weather}\nmodel_mape(gbm, load~., loads, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.2, verbose=TRUE)\n# 2.2\n```\nThis is virtually no different than before.\n\nLet's see how a week-ago loads does.\n\n```{r weekago-gbm-with-weather}\nloads$dayago = NULL\nloads$weekago = apply(loads[,c('load', 'hour', 'day')], 1, function(x) loads[loads$hour==x['hour']&loads$day==(x['day']-7),]$load[1])\nmodel_mape(gbm, load~., loads, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.2, verbose=TRUE)\n```\n\nAbout the same. Let's try adding hour-ago loads for various hour increments.\n```{r hourago-gbm-with-weather}\nloads$weekago = NULL\nloads$hour12.ago = apply(loads[,c('load', 'hour', 'day')], 1, function(x) loads[loads$hour==(x['hour']-12)&loads$day==x['day'],]$load[1])\nmodel_mape(gbm, load~., loads, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.2, verbose=TRUE)\n\nloads$hour12.ago = NULL\nloads$hour6.ago = apply(loads[,c('load', 'hour', 'day')], 1, function(x) loads[loads$hour==(x['hour']-6)&loads$day==x['day'],]$load[1])\nmodel_mape(gbm, load~., loads, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.2, verbose=TRUE)\n\nloads$hour6.ago = NULL\nloads$hour3.ago = apply(loads[,c('load', 'hour', 'day')], 1, function(x) loads[loads$hour==(x['hour']-3)&loads$day==x['day'],]$load[1])\nmodel_mape(gbm, load~., loads, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.2, verbose=TRUE)\n\nloads$hour3.ago = NULL\nloads$hour1.ago = apply(loads[,c('load', 'hour', 'day')], 1, function(x) loads[loads$hour==(x['hour']-1)&loads$day==x['day'],]$load[1])\nmodel_mape(gbm, load~., loads, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.2, verbose=TRUE)\n\nloads$hour1.ago = NULL\n```\n\nThe estimates are best for the case where you have the most recent (1-hour) load.  As expected the accuracy\ndecreases as the time shift increases.  At somewhere around 12-24 hours the estimate with the previous load\nis about the same as the estimate without.  So the model degrades to being just a the long-term load estimate\nwithout knowledge of previous load.\n\n### 10:00AM Load\n\nUnfortunately, we often don't have the 1-hour ago load value.  Specifically, for Fort Collins, they are\nmost interested in the load estimate from 10:00 AM forward.  So let's look at a model that uses the load\nat 10:00 for the daily projection.\n\n```{r 10am-load}\nloads$load.10am = apply(loads[,c('load', 'hour', 'day')], 1, function(x) loads[x['hour']>10&loads$hour==10&loads$day==x['day'],]$load[1])\nmodel_mape(gbm, load~., loads, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.2, verbose=TRUE)\nloads$load.10am = NULL\n```\n\nThe MAPE is between the 3-hour and 6-hour estimates from before which is reasonable since we are doing\nestimates for a 10-hour window between 11:00 and 21:00.  So on average you would expect it to be something\nlike a 5-hour estimate.\n\n## Feed-Forward Load\n\nWhat if we just feed forward estimates from one hour to the next?\nIn order to study this idea, `model_mape` had to be modified to do the feed-forward process.\n\n```{r feed-forward}\nloads$load.prev = apply(loads[,c('load', 'hour', 'day')], 1, function(x) loads[loads$hour==(x['hour']-1)&loads$day==x['day'],]$load[1])\n```\n\nWithout the `feedforward` flag we should get the same MAPE as the `hours.ago` model above. \n```{r without-feed-forward}\nmodel_mape(gbm, load~., loads, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.2, verbose=TRUE)\n```\nAnd with the `feedforward` flag, the error should increase.\n```{r with-feed-forward}\nmodel_mape(gbm, load~., loads, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.2, feedforward=TRUE, verbose=TRUE)\n# 1.9\n```\n\n```{r feed-forward-cleanup}\nloads$load.prev = NULL\n```\n\n### Multiple Training\n\nNext we explore training with multiple instances of every feature vector.\nFor the first example, we take each feature and we add a a `load.prev` sample with the value of the previous load.\nThen we duplicate that record with an `NA` as the value.  That way, we can train the algorithm both for the\ncase where we have a previous load, and the case where we don't.\n\nSo we don't pollute the original data, let's create a new data.frame called `loads.multi`\n```{r loads-multi}\nloads.prev = loads\nloads.prev$load.prev = apply(loads[,c('hour', 'day')], 1, function(x) loads[loads$hour==(x['hour']-1)&loads$day==x['day'],]$load[1])\n\nloads.prev.na = loads.prev\nloads.prev.na$load.prev = NA\n\nloads.multi = rbind(loads.prev.na, loads.prev)\n```\n\nRunning this with the data set that has no previous load we would expect to be around the same MAPE as\nwe got above with no previous loads (approx. 2.2%):\n```{r multi-mape-no-prev}\nmodel_mape(gbm, load~., loads.multi, tdata=loads.prev.na, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.2, verbose=TRUE)\n```\nIdeally using the 1-hour previous load data for testing would get us close to the 1-hour MAPE (approx 1.1%).\n```{r multi-mape-with-prev}\nmodel_mape(gbm, load~., loads.multi, tdata=loads.prev, distribution=\"gaussian\", n.trees=1000, interaction.depth=5, shrinkage=0.2, verbose=TRUE)\n```\nUnfortunately, we don't do as well as before.  This seems to suggest that we cannot achieve the same level of \naccuracy with one model to accomodate estimates that use previous loads.  We might need a different model for each\ntime differential up to some threshold (12-24 hours?).  Or perhaps we can use the feed-forward model up to some\nthreshold then switch to a more general model.\n\n\n## Identifying Important Observations\n\nTo get an idea of the relative importance of each observation, do a summary on the `gbm` output.\n\n```{r gbm-summary}\nsummary(gbm(load~., loads.prev, distribution=\"gaussian\", n.trees=1000, interaction.depth=5, shrinkage=0.2))\n```\n\nFirst let's get the baseline numbers with and without the previous load.\n\n```{r gbm-mape-baseline}\nmodel_mape(gbm, load~., loads.prev, distribution=\"gaussian\", n.trees=1000, interaction.depth=5, shrinkage=0.2, verbose=FALSE)\n# 1.0\nmodel_mape(gbm, load~.-load.prev, loads.prev, distribution=\"gaussian\", n.trees=1000, interaction.depth=5, shrinkage=0.2, verbose=FALSE)\n# 2.2\n```\n\nNow try removing observatrions from the bottom of the list until we see a\nsignificant change in MAPE.  First remove visibility and check the MAPE.\n\n```{r gbm-mape-no-visibilty}\nmodel_mape(gbm, load~.-visibility, loads.prev, distribution=\"gaussian\", n.trees=1000, interaction.depth=5, shrinkage=0.2, verbose=FALSE)\n# 1.0\nmodel_mape(gbm, load~.-visibility-load.prev, loads.prev, distribution=\"gaussian\", n.trees=1000, interaction.depth=5, shrinkage=0.2, verbose=FALSE)\n# 2.2\n```\n\nIt looks like visibility has no discernible effect.  Let's remove more things until we see\na noticeable difference.\n\n```{r gbm-mape-no-sky-cover}\nmodel_mape(gbm, load~.-visibility-sky_cover, loads.prev, distribution=\"gaussian\", n.trees=1000, interaction.depth=5, shrinkage=0.2, verbose=FALSE)\n# 1.0\nmodel_mape(gbm, load~.-visibility-sky_cover-load.prev, loads.prev, distribution=\"gaussian\", n.trees=1000, interaction.depth=5, shrinkage=0.2, verbose=FALSE)\n# 2.2\n```\n\n\n```{r gbm-mape-no-cloud-ceiling}\nmodel_mape(gbm, load~.-visibility-sky_cover-cloud_ceiling, loads.prev, distribution=\"gaussian\", n.trees=1000, interaction.depth=5, shrinkage=0.2, verbose=FALSE)\n# 1.0\nmodel_mape(gbm, load~.-visibility-sky_cover-cloud_ceiling-load.prev, loads.prev, distribution=\"gaussian\", n.trees=1000, interaction.depth=5, shrinkage=0.2, verbose=FALSE)\n# 2.2\n```\n\n\n```{r gbm-mape-no-wind}\nmodel_mape(gbm, load~.-visibility-sky_cover-cloud_ceiling-wind_direction-wind_speed, loads.prev, distribution=\"gaussian\", n.trees=1000, interaction.depth=5, shrinkage=0.2, verbose=FALSE)\n# 1.0\nmodel_mape(gbm, load~.-visibility-sky_cover-cloud_ceiling-wind_direction-wind_speed-load.prev, loads.prev, distribution=\"gaussian\", n.trees=1000, interaction.depth=5, shrinkage=0.2, verbose=FALSE)\n# 2.2\n```\n\n```{r gbm-mape-no-pressure}\nmodel_mape(gbm, load~.-visibility-sky_cover-cloud_ceiling-wind_direction-wind_speed-pressure, loads.prev, distribution=\"gaussian\", n.trees=1000, interaction.depth=5, shrinkage=0.2, verbose=FALSE)\n# 1.0\nmodel_mape(gbm, load~.-visibility-sky_cover-cloud_ceiling-wind_direction-wind_speed-pressure-load.prev, loads.prev, distribution=\"gaussian\", n.trees=1000, interaction.depth=5, shrinkage=0.2, verbose=FALSE)\n# 2.2\n```\n\n```{r gbm-mape-no-day}\nmodel_mape(gbm, load~.-visibility-sky_cover-cloud_ceiling-wind_direction-wind_speed-pressure-day, loads.prev, distribution=\"gaussian\", n.trees=1000, interaction.depth=5, shrinkage=0.2, verbose=FALSE)\n# 1.0\nmodel_mape(gbm, load~.-visibility-sky_cover-cloud_ceiling-wind_direction-wind_speed-pressure-day-load.prev, loads.prev, distribution=\"gaussian\", n.trees=1000, interaction.depth=5, shrinkage=0.2, verbose=FALSE)\n# 2.4\n```\n\nWe finally found something that makes a small difference.  Removing `day` makes a small\ndifference for the long-term forecast, which makes some sense since it is\nthe long-term measure of days since the beginning of time.\nBut it makes no difference for the short-term forecast.  This suggests that we might be \nable to use fewer variables for the short-term forecast.\nHowever, since day is always increasing, it might be better to instead model this\nby decreasing the influence of older observations over time.\n\n\n```{r gbm-mape-no-wday}\nmodel_mape(gbm, load~.-visibility-sky_cover-cloud_ceiling-wind_direction-wind_speed-pressure-day-wday, loads.prev, distribution=\"gaussian\", n.trees=1000, interaction.depth=5, shrinkage=0.2, verbose=FALSE)\n# 1.0\nmodel_mape(gbm, load~.-visibility-sky_cover-cloud_ceiling-wind_direction-wind_speed-pressure-day-wday-load.prev, loads.prev, distribution=\"gaussian\", n.trees=1000, interaction.depth=5, shrinkage=0.2, verbose=FALSE)\n```\n\nAnd now we have our first significant effect on the long-term forecast.\nThis suggests that we need to keep `wday` in the long-term forecast.  And keeping\nit in the short-term forecast has no effect so keeping it might be a decision of\nexpediency.  We might as well keep it for both long-term and short-term forecasts since\nit improves long-term forecasts and has no measurable effect on long-term forecasts.\n\nWe are left with the following observations in order of significance:\n\n * previous load (if available, short-term forecasts only)\n * temperature\n * hour\n * day-of-year (yday)\n * dew point\n * day-of-week (wday)\n \nBreaking this down further by weather and time data.\nThe most important weather data is always in the order of:\n\n  * temperature\n  * dew point\n  \nAnd the most important time data is in the order of:\n\n  * hour\n  * day-of-year\n  * day-of-week\n\n### Tuning the Boosting Model\n\nBased on the last section, let's remove the unused observations and try to tune the `gbm` settings.\n```{r remove-unused}\nloads.basic = loads.prev\nloads.basic$visibility = NULL\nloads.basic$sky_cover = NULL\nloads.basic$cloud_ceiling = NULL\nloads.basic$wind_direction = NULL\nloads.basic$wind_speed = NULL\nloads.basic$pressure = NULL\n```\n\nLet's start with the tunings we have been using up to this point.  We will also tune\nthe short term (with previous load) and long-term models separately.  As much as possible\nwe'd like to tune them the same but that may not always be feasible.\n\nAlso, because the intent is to feed forward the short-term estimates for the most recent load,\nwe will consider use the `feedforward` setting to evaluate the short-term model.\n\n```{r gbm-basic-short}\nmodel_mape(gbm, load~.-day, loads.basic, distribution=\"gaussian\", n.trees=1000, interaction.depth=5, shrinkage=0.2, feedforward=TRUE, verbose=TRUE)\n\nmodel_mape(gbm, load~.-day-load.prev, loads.basic, distribution=\"gaussian\", n.trees=1000, interaction.depth=5, shrinkage=0.2, verbose=TRUE)\n```\n\n#### interaction.depth\n\nIncreasing interaction depth increases the complexity of the interactions between observations.\nHere are runs with the short-term model.\n\n```{r gbm-interactions-short}\nmodel_mape(gbm, load~.-day, loads.basic, distribution=\"gaussian\", n.trees=1000, interaction.depth=3, shrinkage=0.2, feedforward=TRUE, verbose=TRUE)\nmodel_mape(gbm, load~.-day, loads.basic, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.2, feedforward=TRUE, verbose=TRUE)\nmodel_mape(gbm, load~.-day, loads.basic, distribution=\"gaussian\", n.trees=1000, interaction.depth=5, shrinkage=0.2, feedforward=TRUE, verbose=TRUE)\nmodel_mape(gbm, load~.-day, loads.basic, distribution=\"gaussian\", n.trees=1000, interaction.depth=6, shrinkage=0.2, feedforward=TRUE, verbose=TRUE)\nmodel_mape(gbm, load~.-day, loads.basic, distribution=\"gaussian\", n.trees=1000, interaction.depth=7, shrinkage=0.2, feedforward=TRUE, verbose=TRUE)\n```\nThe MAPE decreases slightly as we increase the depth.  Beyond 4 there doesn't seem to be significant improvement.\n\nAnd now, the long-term model:\n\n```{r gbm-interactions-long}\nmodel_mape(gbm, load~.-day-load.prev, loads.basic, distribution=\"gaussian\", n.trees=1000, interaction.depth=3, shrinkage=0.2, verbose=TRUE)\nmodel_mape(gbm, load~.-day-load.prev, loads.basic, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.2, verbose=TRUE)\nmodel_mape(gbm, load~.-day-load.prev, loads.basic, distribution=\"gaussian\", n.trees=1000, interaction.depth=5, shrinkage=0.2, verbose=TRUE)\nmodel_mape(gbm, load~.-day-load.prev, loads.basic, distribution=\"gaussian\", n.trees=1000, interaction.depth=6, shrinkage=0.2, verbose=TRUE)\nmodel_mape(gbm, load~.-day-load.prev, loads.basic, distribution=\"gaussian\", n.trees=1000, interaction.depth=7, shrinkage=0.2, verbose=TRUE)\n```\n\nFor the long-term model, a depth of 4 seems to be best.  So we'll use it for both models.\n\n\n#### shrinkage\n\nWith shrinkage, we want the largest value that gives good performance because a larger\nshrinkage might allow us to use less trees.\n\nFirst the short-term model:\n```{r gbm-shrinkage-short}\nmodel_mape(gbm, load~.-day, loads.basic, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.1, feedforward=TRUE, verbose=TRUE)\nmodel_mape(gbm, load~.-day, loads.basic, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.15, feedforward=TRUE, verbose=TRUE)\nmodel_mape(gbm, load~.-day, loads.basic, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.20, feedforward=TRUE, verbose=TRUE)\nmodel_mape(gbm, load~.-day, loads.basic, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.25, feedforward=TRUE, verbose=TRUE)\nmodel_mape(gbm, load~.-day, loads.basic, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.3, feedforward=TRUE, verbose=TRUE)\n```\n\nIt flattens out between 0.2 and 0.3.  Let's see what the long-term model looks like before deciding.\n\nFor the long-term model:\n\n```{r gbm-shrinkage-long}\nmodel_mape(gbm, load~.-day-load.prev, loads.basic, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.1, verbose=TRUE)\nmodel_mape(gbm, load~.-day-load.prev, loads.basic, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.15, verbose=TRUE)\nmodel_mape(gbm, load~.-day-load.prev, loads.basic, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.20, verbose=TRUE)\nmodel_mape(gbm, load~.-day-load.prev, loads.basic, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.25, verbose=TRUE)\nmodel_mape(gbm, load~.-day-load.prev, loads.basic, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.3, verbose=TRUE)\n```\n\nThis is best from 0.15 to 0.25 so the overlap between short and long-term suggests we choose 0.25.\n\n#### n.trees\n\nWe'd like to use as few trees as we can without increasing MAPE.\n\n```{r gbm-basic-ntrees-short}\nmodel_mape(gbm, load~.-day, loads.basic, distribution=\"gaussian\", n.trees=1500, interaction.depth=4, shrinkage=0.25, feedforward=TRUE, verbose=TRUE)\nmodel_mape(gbm, load~.-day, loads.basic, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.25, feedforward=TRUE, verbose=TRUE)\nmodel_mape(gbm, load~.-day, loads.basic, distribution=\"gaussian\", n.trees=750, interaction.depth=4,shrinkage=0.25, feedforward=TRUE, verbose=TRUE)\nmodel_mape(gbm, load~.-day, loads.basic, distribution=\"gaussian\", n.trees=500, interaction.depth=4, shrinkage=0.25, feedforward=TRUE, verbose=TRUE)\n```\nIt looks like the elbow in this curve is around 1000 trees.\n\n```{r gbm-basic-ntrees-long}\nmodel_mape(gbm, load~.-day-load.prev, loads.basic, distribution=\"gaussian\", n.trees=1500, interaction.depth=4, shrinkage=0.25, verbose=TRUE)\nmodel_mape(gbm, load~.-day-load.prev, loads.basic, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.25, verbose=TRUE)\nmodel_mape(gbm, load~.-day-load.prev, loads.basic, distribution=\"gaussian\", n.trees=750, interaction.depth=4,shrinkage=0.25, verbose=TRUE)\nmodel_mape(gbm, load~.-day-load.prev, loads.basic, distribution=\"gaussian\", n.trees=500, interaction.depth=4, shrinkage=0.25, verbose=TRUE)\n```\n\nIt looks like we could reasonably get away with fewer than 1000 trees for the long-term model.\nBut for consistency, let's stick with 1000.\n\n#### final settings\n\nHere's the settings that seem to work well for short and long-term models\n\n * n.trees = 1000\n * interaction.depth = 4\n * shrinkage = 0.25\n \n## Summary\n\nFor now we will use 2 models to forecast load.  The short-term model will feed forward the\nlast observed load for 12 hours.  After 12 hours we will switch to the long-term model.  Both\nmodels will use the gradient boosting trees algorithm with the settings described in the previous\nsection.\n\n\n\n## Appendix - Generating this Document\n\nIf you are in RStudio and want an HTML version of this document, just click Knit HTML above this pane.  If you want to generate the markdown for the github wiki, keep reading.\n\nFirst, configure the figure path so that the images end up in the images folder with\na relative url in \"analysis/images/\"\n```r\nrequire(knitr)\ngetwd()\nsetwd(\"/your/path/to/advanced-apps.wiki/analysis\")  # change this line to your path\nopts_knit$set(base.dir = '..')\nopts_chunk$set(fig.path = paste0(basename(getwd()), '/images/'))\n```\n\nThen you can generate the markdown for this page using the `knitr` package\n```r\nknit(\"fcu.Rmd\", output=\"Analyzing-FCU-Data-with-R.md\")\n```\n\n","markers":{"markers":{"1":{"id":1,"range":[[162,6],[162,9]],"tailed":true,"reversed":false,"valid":true,"invalidate":"never","persistent":true,"properties":{"type":"selection","editorId":12,"goalBufferRange":null},"deserializer":"Marker"}},"deserializer":"MarkerManager"},"history":{"undoStack":[],"redoStack":[],"deserializer":"History"},"encoding":"utf8","filePath":"/Users/tra/src/advanced-apps.wiki/analysis/fcu.Rmd","modifiedWhenLastPersisted":false,"digestWhenLastPersisted":"f97349e74bf2dcf13514e401cf2d4bed91b89e51","deserializer":"TextBuffer"},{"text":"\nfcu.load = function(timezone='UTC', longmont=FALSE, coalesce=mean, mean.time.offset=-1800) {\n  setwd('~/src/advanced-apps.wiki/analysis/data/fcu')\n  \n  loads = fcu.load.loads(timezone, coalesce)\n  \n  # fill in load.prev\n  loads$load.prev = apply(loads, 1, function(x) loads[(loads$time+3600)==as.POSIXct(x['time'],timezone),]$load[1])\n  \n  weathers = fcu.load.weathers(timezone, coalesce)\n  loads = merge(loads, weathers, by=\"time\")\n  \n  if (longmont) {\n    weathers = fcu.load.weathers(timezone, ext=\"_longmont.csv\")\n    colnames(weathers) = c(\"time\", \"temperature.lm\", \"dew_point.lm\")\n    loads = merge(loads, weathers, by=\"time\")\n  }\n\n  if (identical(coalesce, mean)) {\n    # offset the time in the previous hour (back 30 minutes by default)\n    loads$time = loads$time + mean.time.offset\n  }\n  loads$hour = as.POSIXlt(loads$time)$hour + as.POSIXlt(loads$time)$min/60\n  loads$wday = ((as.POSIXlt(loads$time)$wday + 6) %% 7) + 1  # put sunday at 7 so it's next to saturday\n  loads$yday = as.POSIXlt(loads$time)$yday\n  loads$day = as.Date(loads$time) - as.Date(\"2011-01-01\")\n  \n  loads$time = NULL\n  \n  loads  \n}\n\nfcu.load.loads = function(timezone, coalesce) {\n  # set working directory to wherever you have unpacked ft-collins-history-2011-2012.tar.gz\n  \n  loads = NULL\n  for (year in 2011:2013) {\n    for (month in 1:12) {\n      filename = paste0('./csvs/', month, '-', year, '.csv')\n      cat(\"loading \", filename, \"\\n\")\n      data = read.csv(filename)\n      # convert garbage characters to integer\n      if (is.factor(data$value)) {\n        data$value = as.integer(as.character(data$value))\n      }\n      # get rid of stuff that looks bad\n      data = data[!is.na(data$value) & data$value>0,]\n      \n      # ignore minutes:seconds-tz part, truncate to the hour, then add hour to get to hour-ending time\n      # also, keep times in MST to avoid hour shift at DST boundaries\n      # (considered using UTC but that would put peaks around 0300 which will probably be bad)\n      stimes = gsub('(T..):..:..(-..):(..)$', '\\\\1\\\\2\\\\3', data$collectedAt)\n      hours = strptime(stimes, \"%Y-%m-%dT%H%z\", tz=timezone)+3600\n      loads.month = aggregate(data$value, list(hours), coalesce)\n      loads = rbind(loads, loads.month)\n      rm(stimes); rm(hours) ; rm(data) ; rm(loads.month)\n    }\n  }\n  \n  # do this after looping over the data\n  colnames(loads) = c(\"time\", \"load\")\n  loads\n}\n\nfcu.load.weathers = function(timezone, coalesce, ext=\"_ft_collins.csv\") {\n  weathers = NULL\n  for (year in 2011:2013) {\n    filename = paste0('./', year, ext)\n    cat(\"loading \", filename, \"\\n\")\n    data = read.csv(filename)\n    \n    numeric_fields = c('temperature', 'dew_point', 'wind_speed', 'wind_direction', 'cloud_ceiling', 'visibility', 'pressure')\n    for (field in numeric_fields) {\n      data[field] = as.numeric(as.character(data[field][,1]))\n    }\n    \n    # ignore minutes:seconds-tz part, truncate to the hour, then add hour to get to hour-ending time\n    # also, keep times in MST to avoid hour shift at DST boundaries\n    stimes = gsub('(T..):..:..([+-]....)$', '\\\\1\\\\2', data$timestamp)\n    hours = strptime(stimes, \"%Y-%m-%dT%H%z\", tz=timezone)+3600\n    weathers.hourly = aggregate(data[,numeric_fields], list(hours), FUN=coalesce)\n    \n    # take the median sky_cover value\n    weathers.hourly$sky_cover = as.factor(levels(data$sky_cover)[aggregate(as.integer(data$sky_cover), list(hours), median)[,2]])\n    \n    weathers = rbind(weathers, weathers.hourly)\n    rm(stimes); rm(hours) ; rm(data) ; rm(weathers.hourly)\n  }\n  colnames(weathers)[1] = \"time\"\n  \n  # remove unused columns\n  weathers$visibility = NULL\n  weathers$sky_cover = NULL\n  weathers$cloud_ceiling = NULL\n  weathers$wind_direction = NULL\n  weathers$wind_speed = NULL\n  weathers$pressure = NULL\n  \n  weathers\n}\n\nlast = function(x) {\n  tail(x,1)\n}\n","markers":{"markers":{"1":{"id":1,"range":[[0,0],[0,0]],"tailed":false,"reversed":false,"valid":true,"invalidate":"never","persistent":true,"properties":{"type":"selection","editorId":36},"deserializer":"Marker"}},"deserializer":"MarkerManager"},"history":{"undoStack":[],"redoStack":[],"deserializer":"History"},"encoding":"utf8","filePath":"/Users/tra/src/advanced-apps.wiki/analysis/fcu.R","modifiedWhenLastPersisted":false,"digestWhenLastPersisted":"9b0c7fd68f60aab0b1cf2782c0343cb3230a07d9","deserializer":"TextBuffer"}],"deserializer":"Project"},"workspace":{"paneContainer":{"root":{"id":3,"items":[{"id":4,"softTabs":true,"displayBuffer":{"id":5,"softWrapped":true,"editorWidthInChars":null,"scrollTop":0,"scrollLeft":0,"tokenizedBuffer":{"bufferPath":"/Users/tra/src/advanced-apps.wiki/analysis/Analyzing-FCU-Data-with-R.md","invisibles":null,"deserializer":"TokenizedBuffer"},"invisibles":null,"deserializer":"DisplayBuffer"},"deserializer":"TextEditor"},{"id":8,"softTabs":true,"displayBuffer":{"id":9,"softWrapped":true,"editorWidthInChars":null,"scrollTop":43871,"scrollLeft":0,"tokenizedBuffer":{"bufferPath":"/Users/tra/src/advanced-apps.wiki/analysis/Analyzing-PJM-Data-with-R.md","invisibles":null,"deserializer":"TokenizedBuffer"},"invisibles":null,"deserializer":"DisplayBuffer"},"deserializer":"TextEditor"},{"id":36,"softTabs":true,"displayBuffer":{"id":37,"softWrapped":false,"editorWidthInChars":null,"scrollTop":0,"scrollLeft":0,"tokenizedBuffer":{"bufferPath":"/Users/tra/src/advanced-apps.wiki/analysis/fcu.R","invisibles":null,"deserializer":"TokenizedBuffer"},"invisibles":null,"deserializer":"DisplayBuffer"},"deserializer":"TextEditor"},{"id":12,"softTabs":true,"displayBuffer":{"id":13,"softWrapped":true,"editorWidthInChars":null,"scrollTop":4121,"scrollLeft":0,"tokenizedBuffer":{"bufferPath":"/Users/tra/src/advanced-apps.wiki/analysis/fcu.Rmd","invisibles":null,"deserializer":"TokenizedBuffer"},"invisibles":null,"deserializer":"DisplayBuffer"},"deserializer":"TextEditor"}],"activeItemURI":"/Users/tra/src/advanced-apps.wiki/analysis/fcu.Rmd","focused":true,"deserializer":"Pane"},"activePaneId":3,"deserializer":"PaneContainer","version":1},"fullScreen":false,"packagesWithActiveGrammars":["language-gfm","language-hyperlink","language-todo"],"deserializer":"Workspace"},"grammars":{"deserializer":"GrammarRegistry","grammarOverridesByPath":{}},"packageStates":{"fuzzy-finder":{"/Users/tra/src/advanced-apps.wiki/analysis/Analyzing-FCU-Data-with-R.md":1426112949800,"/Users/tra/src/advanced-apps.wiki/analysis/Analyzing-PJM-Data-with-R.md":1426112953300,"/Users/tra/src/advanced-apps.wiki/analysis/fcu.R":1426112955445,"/Users/tra/src/advanced-apps.wiki/analysis/fcu.Rmd":1426112956943},"keybinding-resolver":{"attached":false},"metrics":{"sessionLength":171461338},"tree-view":{"directoryExpansionStates":[{"data":{}}],"selectedPath":"/Users/tra/src/advanced-apps.wiki/analysis/fcu.Rmd","hasFocus":false,"attached":true,"scrollLeft":0,"scrollTop":0,"width":310},"find-and-replace":{"viewState":"","modelState":{"useRegex":false,"inCurrentSelection":false,"caseSensitive":false,"wholeWord":false},"projectViewState":"","resultsModelState":{"useRegex":false,"caseSensitive":false},"findHistory":["opts_chunk","gbm"],"replaceHistory":[],"pathsHistory":[]}}}